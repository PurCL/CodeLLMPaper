{
    "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "han-etal-2024-archcode",
        "author": "Han, Hojae and Kim, Jaejin and Yoo, Jaeseok and Lee, Youngwon and Hwang, Seung-won",
        "booktitle": "ACL2024",
        "title": "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores.Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs\u2019 non-functional requirements in code generation, demonstrating ARCHCODE\u2019s superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.730",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-hirope",
        "author": "Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.735",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-codeagent",
        "author": "Zhang, Kechi and Li, Jia and Li, Ge and Shi, Xianjie and Jin, Zhi",
        "booktitle": "ACL2024",
        "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools\u2019 usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent\u2019s adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent\u2019s robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.737",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models": {
        "type": "INPROCEEDINGS",
        "key": "riddell-etal-2024-quantifying",
        "author": "Riddell, Martin and Ni, Ansong and Cohan, Arman",
        "booktitle": "ACL2024",
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. While recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of LLMs in programming contexts. In this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. We also conduct extensive analysis on the factors that affect model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.761",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "ChatDev: Communicative Agents for Software Development": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-chatdev",
        "author": "Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "ChatDev: Communicative Agents for Software Development",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "keywords": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.810",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents": {
        "type": "INPROCEEDINGS",
        "key": "trivedi-etal-2024-appworld",
        "author": "Trivedi, Harsh and Khot, Tushar and Hartmann, Mareike and Manku, Ruskin and Dong, Vinty and Li, Edward and Gupta, Shashank and Sabharwal, Ashish and Balasubramanian, Niranjan",
        "booktitle": "ACL2024",
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our \u2018normal\u2019 tasks and ~30% of \u2018challenge\u2019 tasks, while other models solve at least 16% fewer. This highlights the benchmark\u2019s difficulty and AppWorld\u2019s potential to push the frontiers of interactive coding agents.",
        "keywords": [
            "benchmark",
            "agent design"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.850",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "On Improving Repository-Level Code QA for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "strich-etal-2024-improving",
        "author": "Strich, Jan and Schneider, Florian and Nikishina, Irina and Biemann, Chris",
        "booktitle": "ACL2024",
        "title": "On Improving Repository-Level Code QA for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) such as ChatGPT, GitHub Copilot, Llama, or Mistral assist programmers as copilots and knowledge sources to make the coding process faster and more efficient. This paper aims to improve the copilot performance by implementing different self-alignment processes and retrieval-augmented generation (RAG) pipelines, as well as their combination. To test the effectiveness of all approaches, we create a dataset and apply a model-based evaluation, using LLM as a judge. It is designed to check the model\u2019s abilities to understand the source code semantics, the dependency between files, and the overall meta-information about the repository. We also compare our approach with other existing solutions, e.g. ChatGPT-3.5, and evaluate on the existing benchmarks. Code and dataset are available online (https://anonymous.4open.science/r/ma_llm-382D).",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-srw.28",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks": {
        "type": "INPROCEEDINGS",
        "key": "10646663",
        "author": "Ullah, Saad and Han, Mingji and Pujar, Saurabh and Pearce, Hammond and Coskun, Ayse and Stringhini, Gianluca",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "862-880",
        "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like \u2018PaLM2\u2019 and \u2018GPT-4\u2019: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",
        "keywords": [
            "static analysis",
            "bug detection",
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/SP54263.2024.00210",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices": {
        "type": "INPROCEEDINGS",
        "key": "10646659",
        "author": "Wang, Jincheng and Yu, Le and Luo, Xiapu",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "LLMIF: Augmented Large Language Model for Fuzzing IoT Devices",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "881-896",
        "abstract": "Despite the efficacy of fuzzing in verifying the implementation correctness of network protocols, existing IoT protocol fuzzing approaches grapple with several limitations, including obfuscated message formats, unresolved message dependencies, and a lack of evaluations on the testing cases. These limitations significantly curtail the capabilities of IoT fuzzers in vulnerability identification. In this work, we show that the protocol specification contains fruitful descriptions of protocol messages, which can be used to overcome the above limitations and guide IoT protocol fuzzing. To automate the specification analysis, we augment the large language model with the specification contents, and drive it to perform two tasks (i.e., protocol information extraction, and device response reasoning). We further design and implement a fuzzing algorithm, LLMIF, which incorporates the LLM into IoT fuzzing. Finally, we select Zigbee as the target protocol and initiate comprehensive evaluations. The evaluation result shows that LLMIF successfully addressed the above limitations. Compared with the existing Zigbee fuzzers, it increases the protocol message coverage and code coverage by 55.2% and 53.9%, respectively. Besides the enhanced coverage, LLMIF unearthed 11 vulnerabilities on real-world Zigbee devices, which include eight previously unknown vulnerabilities. Seven of them are not covered by the existing Zigbee fuzzers.",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/SP54263.2024.00211",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models": {
        "type": "INPROCEEDINGS",
        "key": "10646865",
        "author": "Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert",
        "booktitle": "2024 IEEE Symposium on Security and Privacy (SP)",
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "1122-1140",
        "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model\u2019s training by injecting malicious data. Poisoning attacks could be designed to influence the model\u2019s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TrojanPuzzle, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TrojanPuzzle robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
        "keywords": [
            "code model security",
            "attack",
            "code generation",
            "code completion"
        ],
        "doi": "10.1109/SP54263.2024.00140",
        "ISSN": "2375-1207",
        "month": "May",
        "venue": "S&P2024"
    },
    "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694987",
        "author": "Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan",
        "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694987",
        "doi": "10.1145/3691620.3694987",
        "abstract": "Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28\\% on EM, 13\\% on BLEU, and 6.8\\% on CodeBLEU.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "65\u201377",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694997",
        "author": "Lu, Jiawei and Wang, Haoye and Liu, Zhongxin and Liang, Keyu and Bao, Lingfeng and Yang, Xiaohu",
        "title": "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694997",
        "doi": "10.1145/3691620.3694997",
        "abstract": "Recent studies proposed to leverage large language models (LLMs) with In-Context Learning (ICL) to handle code intelligence tasks without fine-tuning. ICL employs task instructions and a set of examples as demonstrations to guide the model in generating accurate answers without updating its parameters. While ICL has proven effective for code intelligence tasks, its performance heavily relies on the selected examples. Previous work has achieved some success in using BM25 to retrieve examples for code intelligence tasks. However, existing approaches lack the ability to understand the semantic and structural information of queries, resulting in less helpful demonstrations. Moreover, they do not adapt well to the complex and dynamic nature of user queries in diverse domains. In this paper, we introduce a novel approach named Instructive Code Retriever (ICR), which is designed to retrieve examples that enhance model inference across various code intelligence tasks and datasets. We enable ICR to learn the semantic and structural information of the corpus by a tree-based loss function. To better understand the correlation between queries and examples, we incorporate the feedback from LLMs to guide the training of the retriever. Experimental results demonstrate that our retriever significantly outperforms state-of-the-art approaches. We evaluate our model's effectiveness on various tasks, i.e., code summarization, program synthesis, and bug fixing. Compared to previous state-of-the-art algorithms, our method achieved improvements of 50.0\\% and 90.0\\% in terms of BLEU-4 for two code summarization datasets, 74.6\\% CodeBLEU on program synthesis dataset, and increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "191\u2013203",
        "numpages": "13",
        "keywords": [
            "prompt strategy",
            "RAG"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Code Changes Practically with Small-Scale Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3694999",
        "author": "Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian",
        "title": "Understanding Code Changes Practically with Small-Scale Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3694999",
        "doi": "10.1145/3691620.3694999",
        "abstract": "Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with \u226570b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "216\u2013228",
        "numpages": "13",
        "keywords": [
            "code summarization",
            "code model",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695000",
        "author": "Sun, Zhihong and Wan, Yao and Li, Jia and Zhang, Hongyu and Jin, Zhi and Li, Ge and Lyu, Chen",
        "title": "Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695000",
        "doi": "10.1145/3691620.3695000",
        "abstract": "Large Language Models (LLMs), such as GPT-4, StarCoder, and Code Llama, are transforming the way developers approach programming by automatically generating code based on given contexts, such as natural language descriptions or incomplete surrounding code. Despite advancements, generating syntactically and semantically correct code remains challenging, especially for complex programming tasks. Existing approaches typically generate multiple candidate solutions using LLMs to increase the likelihood of producing correct code. However, selecting the correct code from these candidates --- a process known as code ranking --- remains a major challenge. Current research on code ranking can be categorized into execution-based and non-execution-based methods. Execution-based methods, although effective, encounter notable limitations, such as scarcity of quality unit tests and security risks. Non-execution-based methods like CodeRanker, which rely solely on classification labels to train a code ranker, struggle to capture subtle errors and provide detailed error insights. Recognizing the strengths and limitations of both approaches, we propose a new method that integrates the advantages of execution-based and non-execution-based techniques. The key insight of our work is that an effective code ranker is expected to truly comprehend the underlying causes of erroneous code, as relying solely on classification labels is insufficient. Inspired by this, this paper puts forward RankEF, an innovative approach for code ranking that leverages execution feedback. RankEF employs multi-task learning to integrate code classification with execution feedback generation. This approach enables the model to understand the reasons behind incorrect code, distinguishing between correct and incorrect solutions without the need to execute the code during the ranking phase. Experiments on three code generation benchmarks---APPS, MBPP, and HumanEval---demonstrate that RankEF significantly outperforms the state-of-the-art CodeRanker, achieving relative improvements of +30.97\\%, +31.43\\%, and +19.51\\% in Pass@1, Pass@2, and Pass@5 on APPS test, respectively.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "229\u2013241",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695010",
        "author": "Zhang, Yichi and Liu, Zixi and Feng, Yang and Xu, Baowen",
        "title": "Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695010",
        "doi": "10.1145/3691620.3695010",
        "abstract": "Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust's program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs' ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "356\u2013366",
        "numpages": "11",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695013",
        "author": "Wu, Yulun and Wen, Ming and Yu, Zeliang and Guo, Xiaochen and Jin, Hai",
        "title": "Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695013",
        "doi": "10.1145/3691620.3695013",
        "abstract": "Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge.Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "393\u2013405",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695014",
        "author": "Wu, Guangyuan and Cao, Weining and Yao, Yuan and Wei, Hengfeng and Chen, Taolue and Ma, Xiaoxing",
        "title": "LLM Meets Bounded Model Checking: Neuro-symbolic Loop Invariant Inference",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695014",
        "doi": "10.1145/3691620.3695014",
        "abstract": "Loop invariant inference, a key component in program verification, is a challenging task due to the inherent undecidability and complex loop behaviors in practice. Recently, machine learning based techniques have demonstrated impressive performance in generating loop invariants automatically. However, these methods highly rely on the labeled training data, and are intrinsically random and uncertain, leading to unstable performance. In this paper, we investigate a synergy of large language models (LLMs) and bounded model checking (BMC) to address these issues. The key observation is that, although LLMs may not be able to return the correct loop invariant in one response, they usually can provide all individual predicates of the correct loop invariant in multiple responses. To this end, we propose a \"query-filter-reassemble\" strategy, namely, we first leverage the language generation power of LLMs to produce a set of candidate invariants, where training data is not needed. Then, we employ BMC to identify valid predicates from these candidate invariants, which are assembled to produce new candidate invariants and checked by off-the-shelf SMT solvers. The feedback is incorporated into the prompt for the next round of LLM querying. We expand the existing benchmark of 133 programs to 316 programs, providing a more comprehensive testing ground. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art techniques, successfully generating 309 loop invariants out of 316 cases, whereas the existing baseline methods are only able to tackle 219 programs at best. The code is publicly available at https://github.com/SoftWiser-group/LaM4Inv.git.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "406\u2013417",
        "numpages": "12",
        "keywords": [
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic-Enhanced Indirect Call Analysis with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695016",
        "author": "Cheng, Baijun and Zhang, Cen and Wang, Kailong and Shi, Ling and Liu, Yang and Wang, Haoyu and Guo, Yao and Li, Ding and Chen, Xiangqun",
        "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695016",
        "doi": "10.1145/3691620.3695016",
        "abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "430\u2013442",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "WaDec: Decompiling WebAssembly Using Large Language Model": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695020",
        "author": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
        "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695020",
        "doi": "10.1145/3691620.3695020",
        "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\\%, a dramatic 97\\% reduction compared to the state-of-the-art's 116.94\\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\\%, a re-execution rate of 43.55\\%, and an output consistency of 27.15\\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\\%, cyclomatic complexity by 8\\%, and cosine similarity by 41\\%, achieving an average code similarity above 50\\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "481\u2013492",
        "numpages": "12",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model",
            "source code model"
        ]
    },
    "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695054",
        "author": "Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang",
        "title": "GraphCoder: Enhancing Repository-Level Code Completion via Coarse-to-fine Retrieval Based on Code Context Graph",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695054",
        "doi": "10.1145/3691620.3695054",
        "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "570\u2013581",
        "numpages": "12",
        "keywords": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695055",
        "author": "Wu, Cong and Chen, Jing and Wang, Ziwei and Liang, Ruichao and Du, Ruiying",
        "title": "Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695055",
        "doi": "10.1145/3691620.3695055",
        "abstract": "Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06\\% with GPT-3.5-turbo, 93.91\\% with LLAMA3, and 94.27\\% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0\\% and a false positive rate of 0.29\\%. These results highlight PonziSleuth's capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "582\u2013593",
        "numpages": "12",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695062",
        "author": "Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi",
        "title": "FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695062",
        "doi": "10.1145/3691620.3695062",
        "abstract": "Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46\\% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\\texttimes{} compared to the autoregressive decoding algorithm.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "669\u2013680",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695063",
        "author": "Yu, Xinran and Li, Chun and Pan, Minxue and Li, Xuandong",
        "title": "DroidCoder: Enhanced Android Code Completion with Context-Enriched Retrieval-Augmented Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695063",
        "doi": "10.1145/3691620.3695063",
        "abstract": "Android is the most popular mobile operating system. However, Android development requires extensive coding, especially for unique features such as lifecycle callbacks and UI widgets. Existing code completion methods typically utilize Retrieval-Augmented Generation (RAG) to provide contextual information for pre-trained code large language models (Code LLMs) to perform completion. Despite considerable progress in these methods, their effectiveness in Android development remains limited. This is because the features of Android development make it challenging for existing retrieval mechanisms to extract sufficient context effectively. In response, we propose DroidCoder, a novel Android code completion framework that employs Android development features and contextual information of code snippets to enrich RAG. It also incorporates a specifically designed loss function to fine-tune the model, enabling it to better utilize context-enhanced RAG for Android code completion. We evaluated our method on three base models and different types of applications, comparing it with two state-of-the-art code completion methods. The experimental results demonstrate that our method significantly outperforms the baselines at line-level and multi-line-level code completion and improves the quality of the completed code.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "681\u2013693",
        "numpages": "13",
        "keywords": [
            "code generation",
            "code completion",
            "prompting strategy",
            "RAG"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695066",
        "author": "Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang",
        "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695066",
        "doi": "10.1145/3691620.3695066",
        "abstract": "Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "719\u2013731",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "code model",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695068",
        "author": "Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695068",
        "doi": "10.1145/3691620.3695068",
        "abstract": "Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8\\% in precision, 2.5\\% in recall, and 18.5\\% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68\\% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "745\u2013757",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "An Empirical Study to Evaluate AIGC Detectors on Code Content": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695468",
        "author": "JianWang and Liu, Shangqing and Xie, Xiaofei and Li, Yi",
        "title": "An Empirical Study to Evaluate AIGC Detectors on Code Content",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695468",
        "doi": "10.1145/3691620.3695468",
        "abstract": "Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored.To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&amp;A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "844\u2013856",
        "numpages": "13",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695470",
        "author": "Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang",
        "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695470",
        "doi": "10.1145/3691620.3695470",
        "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8\\% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3\\% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92\\%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24\\% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "870\u2013882",
        "numpages": "13",
        "keywords": [
            "benchmark",
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695480",
        "author": "Chen, Jiachi and Zhong, Qingyuan and Wang, Yanlin and Ning, Kaiwen and Liu, Yongkun and Xu, Zenan and Zhao, Zhe and Chen, Ting and Zheng, Zibin",
        "title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695480",
        "doi": "10.1145/3691620.3695480",
        "abstract": "Warning: Please note that this article contains potential harmful or offensive content. This content is only for the evaluating and analysis of LLMs and does not imply any intention to promote criminal activities.The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36\\% in text-to-code scenario and 11.52\\% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71\\%; ChatGPT-4 has a refusal rate of only 35.73\\%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "995\u20131006",
        "numpages": "12",
        "keywords": [
            "benchmark",
            "code model security",
            "attack"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Contextualized Data-Wrangling Code Generation in Computational Notebooks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695503",
        "author": "Huang, Junjie and Guo, Daya and Wang, Chenglong and Gu, Jiazhen and Lu, Shuai and Inala, Jeevana Priya and Yan, Cong and Gao, Jianfeng and Duan, Nan and Lyu, Michael R.",
        "title": "Contextualized Data-Wrangling Code Generation in Computational Notebooks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695503",
        "doi": "10.1145/3691620.3695503",
        "abstract": "Data wrangling, the process of preparing raw data for further analysis in computational notebooks, is a crucial yet time-consuming step in data science. Code generation has the potential to automate the data wrangling process to reduce analysts' overhead by translating user intents into executable code. Precisely generating data wrangling code necessitates a comprehensive consideration of the rich context present in notebooks, including textual context, code context and data context. However, notebooks often interleave multiple non-linear analysis tasks into linear sequence of code blocks, where the contextual dependencies are not clearly reflected. Directly training models with source code blocks fails to fully exploit the contexts for accurate wrangling code generation.To bridge the gap, we aim to construct a high quality datasets with clear and rich contexts to help training models for data wrangling code generation tasks. In this work, we first propose an automated approach, CoCoMine to mine data-wrangling code generation examples with clear multi-modal contextual dependency. It first adopts data flow analysis to identify the code blocks containing data wrangling codes. Then, CoCoMine extracts the contextualized data-wrangling code examples through tracing and replaying notebooks. With CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the effectiveness of our dataset, we finetune a range of pretrained code models and prompt various large language models on our task. Furthermore, we also propose Data-Coder, which encodes data context and code&amp;textual contexts separately to enhance code generation. Experiment results demonstrate the significance of incorporating data context in data-wrangling code generation and the effectiveness of our model. We release code and data at https://github.com/Jun-jie-Huang/CoCoNote.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695505",
        "author": "Qu, Muzi and Liu, Jie and Kang, Liangyi and Wang, Shuai and Ye, Dan and Huang, Tao",
        "title": "Dynamic Scoring Code Token Tree: A Novel Decoding Strategy for Generating High-Performance Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695505",
        "doi": "10.1145/3691620.3695505",
        "abstract": "Within the realms of scientific computing, large-scale data processing, and artificial intelligence-powered computation, disparities in performance, which originate from differing code implementations, directly influence the practicality of the code. Although existing works tried to utilize code knowledge to enhance the execution performance of codes generated by large language models, they neglect code evaluation outcomes which directly refer to the code execution details, resulting in inefficient computation. To address this issue, we propose DSCT-Decode, an innovative adaptive decoding strategy for large language models, that employs a data structure named 'Code Token Tree' (CTT), which guides token selection based on code evaluation outcomes. DSCT-Decode assesses generated code across three dimensions---correctness, performance, and similarity---and utilizes a dynamic penalty-based boundary intersection method to compute multi-objective scores, which are then used to adjust the scores of nodes in the CTT during backpropagation. By maintaining a balance between exploration, through token selection probabilities, and exploitation, through multi-objective scoring, DSCT-Decode effectively navigates the code space to swiftly identify high-performance code solutions. To substantiate our framework, we developed a new benchmark, big-DS-1000, which is an extension of DS-1000. This benchmark is the first of its kind to specifically evaluate code generation methods based on execution performance. Comparative evaluations with leading large language models, such as CodeLlama and GPT-4, show that our framework achieves an average performance enhancement of nearly 30\\%. Furthermore, 30\\% of the codes exhibited a performance improvement of more than 20\\%, underscoring the effectiveness and potential of our framework for practical applications.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1308\u20131318",
        "numpages": "11",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695506",
        "author": "Zhang, Huan and Cheng, Wei and Wu, Yuhan and Hu, Wei",
        "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695506",
        "doi": "10.1145/3691620.3695506",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00\\%--162.43\\% compared to prompting LLMs directly.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1319\u20131331",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695508",
        "author": "Wu, Di and Mu, Fangwen and Shi, Lin and Guo, Zhaoqiang and Liu, Kui and Zhuang, Weiguang and Zhong, Yuqi and Zhang, Li",
        "title": "iSMELL: Assembling LLMs with Expert Toolsets for Code Smell Detection and Refactoring",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695508",
        "doi": "10.1145/3691620.3695508",
        "abstract": "Detecting and refactoring code smells is challenging, laborious, and sustaining. Although large language models have demonstrated potential in identifying various types of code smells, they also have limitations such as input-output token restrictions, difficulty in accessing repository-level knowledge, and performing dynamic source code analysis. Existing learning-based methods or commercial expert toolsets have advantages in handling complex smells. They can analyze project structures and contextual information in-depth, access global code repositories, and utilize advanced code analysis techniques. However, these toolsets are often designed for specific types and patterns of code smells and can only address fixed smells, lacking flexibility and scalability. To resolve that problem, we propose iSMELL, an ensemble approach that employs various code smell detection toolsets via Mixture of Experts (MoE) architecture for comprehensive code smell detection, and enhances the LLMs with the detection results from expert toolsets for refactoring those identified code smells. First, we train a MoE model that, based on input code vectors, outputs the most suitable expert tool for identifying each type of smell. Then, we select the recommended toolsets for code smell detection and obtain their results. Finally, we equip the prompts with the detection results from the expert toolsets, thereby enhancing the refactoring capability of LLMs for code with existing smells, enabling them to provide different solutions based on the type of smell. We evaluate our approach on detecting and refactoring three classical and complex code smells, i.e., Refused Bequest, God Class, and Feature Envy. The results show that, by adopting seven expert code smell toolsets, iSMELL achieved an average F1 score of 75.17\\% on code smell detection, outperforming LLMs baselines by an increase of 35.05\\% in F1 score. We further evaluate the code refactored by the enhanced LLM. The quantitative and human evaluation results show that iSMELL could improve code quality metrics and conduct satisfactory refactoring toward the identified code smells. We believe that our proposed solution could provide new insights into better leveraging LLMs and existing approaches to resolving complex software tasks.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1345\u20131357",
        "numpages": "13",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program repair",
            "static analysis",
            "bug detection"
        ]
    },
    "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695512",
        "author": "Pirzada, Muhammad A. A. and Reger, Giles and Bhayat, Ahmed and Cordeiro, Lucas C.",
        "title": "LLM-Generated Invariants for Bounded Model Checking Without Loop Unrolling",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695512",
        "doi": "10.1145/3691620.3695512",
        "abstract": "We investigate a modification of the classical Bounded Model Checking (BMC) procedure that does not handle loops through unrolling but via modifications to the control flow graph (CFG). A portion of the CFG representing a loop is replaced by a node asserting invariants of the loop. We generate these invariants using Large Language Models (LLMs) and use a first-order theorem prover to ensure the correctness of the generated statements. We thus transform programs to loop-free variants in a sound manner. Our experimental results show that the resulting tool, ESBMC ibmc, is competitive with state-of-the-art formal verifiers for programs with unbounded loops, significantly improving the number of programs verified by the industrial-strength software verifier ESBMC and verifying programs that state-of-the-art software verifiers such as SeaHorn and VeriAbs could not.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1395\u20131407",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "program verification",
            "invariant generation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695524",
        "author": "Zhu, Ming and Karim, Mohimenul and Lourentzou, Ismini and Yao, Daphne",
        "title": "Semi-Supervised Code Translation Overcoming the Scarcity of Parallel Code Data",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695524",
        "doi": "10.1145/3691620.3695524",
        "abstract": "Neural code translation is the task of converting source code from one programming language to another. One of the main challenges is the scarcity of parallel code data, which hinders the ability of translation models to learn accurate cross-language alignments. In this paper, we introduce MIRACLE, a semi-supervised approach that improves code translation through synthesizing high-quality parallel code data and curriculum learning on code data with ascending alignment levels. MIRACLE leverages static analysis and compilation to generate synthetic parallel code datasets with enhanced quality and alignment to address the challenge of data scarcity. We evaluate the proposed method along with strong baselines including instruction-tuned Large Language Models (LLMs) for code. Our analysis reveals that LLMs pre-trained on open-source code data, regardless of their size, suffer from the \"shallow translation\" problem. This issue arises when translated code copies keywords, statements, and even code blocks from the source language, leading to compilation and runtime errors. Extensive experiments demonstrate that our method significantly mitigates this issue, enhancing code translation performance across multiple models in C++, Java, Python, and C. Remarkably, MIRACLE outperforms code LLMs that are ten times larger in size. MIRACLE also achieves up to a 43\\% improvement in C code translation with fewer than 150 annotated examples.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1545\u20131556",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Test-Driven Development and LLM-based Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695527",
        "author": "Mathews, Noble Saji and Nagappan, Meiyappan",
        "title": "Test-Driven Development and LLM-based Code Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695527",
        "doi": "10.1145/3691620.3695527",
        "abstract": "Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1583\u20131594",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "On the Evaluation of Large Language Models in Unit Test Generation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695529",
        "author": "Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie",
        "title": "On the Evaluation of Large Language Models in Unit Test Generation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695529",
        "doi": "10.1145/3691620.3695529",
        "abstract": "Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1607\u20131619",
        "numpages": "13",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695536",
        "author": "Chen, Mouxiang and Liu, Zhongxin and Tao, He and Hong, Yusu and Lo, David and Xia, Xin and Sun, Jianling",
        "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695536",
        "doi": "10.1145/3691620.3695536",
        "abstract": "Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy \u212c4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50\\% over the strongest heuristic and 246\\% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1693\u20131705",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695552",
        "author": "Feng, Jia and Liu, Jiachen and Gao, Cuiyun and Chong, Chun Yong and Wang, Chaozheng and Gao, Shan and Xia, Xin",
        "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695552",
        "doi": "10.1145/3691620.3695552",
        "abstract": "In recent years, with the widespread attention of academia and industry on the application of large language models (LLMs) to code-related tasks, an increasing number of large code models (LCMs) have been proposed and corresponding evaluation benchmarks have continually emerged. Although existing evaluation benchmarks are helpful for comparing different LCMs, they may not reflect the performance of LCMs in various development scenarios. Specifically, they might evaluate model performance in only one type of scenario (e.g., code generation or code completion), whereas real development contexts are diverse and may involve multiple tasks such as code generation, code completion, API recommendation, and test function generation. Additionally, the questions may not originate from actual development practices, failing to capture the programming challenges faced by developers during the development process.To address the aforementioned issues, we propose Complex-CodeEval, a new benchmark for evaluating the performance of LCMs in various development scenarios. ComplexCodeEval includes 3,897 Java samples from 1,055 high-star GitHub repositories and 7,184 Python samples from 2,107 high-star repositories. Each function sample in ComplexCodeEval contains multiple annotations (e.g., function signatures, docstrings and reference APIs) to accommodate various downstream tasks. Furthermore, to better reflect diverse development scenarios, each function sample is required to originate from a repository that depends on at least one selected library (based on popularity), and each function sample must invoke at least one API from the selected library. Additionally, each function sample has multiple timestamps to avoid data leakage. Based on ComplexCodeEval, we evaluate the performance of ten LCMs across four tasks (i.e., code generation, code completion, API recommendation, and test case generation) to explore their performance in complex development environments. Furthermore, we conduct an in-depth analysis of the impact of context and data leakage on model performance. Our experimental results reveal several key findings. For instance, LCMs exhibit varying performance across different coding tasks. Additionally, rich contextual information can greatly enhance the performance of LCMs. Moreover, using leaked data for evaluation may lead to an overestimation of model performance, resulting in inaccurate evaluation outcomes that deviate from the performance in practice.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1895\u20131906",
        "numpages": "12",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Understanding Developer-Analyzer Interactions in Code Reviews": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695257",
        "author": "Schaef, Martin and Cirisci, Berk and Luo, Linghui and Mansur, Muhammad Numair and Tripp, Omer and Sanchez, Daniel and Zhou, Qiang and Zafar, Muhammad Bilal",
        "title": "Understanding Developer-Analyzer Interactions in Code Reviews",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695257",
        "doi": "10.1145/3691620.3695257",
        "abstract": "Static code analyzers are now a common part of the codereview process. These automated tools integrate into the code review process by commenting on code changes and suggesting improvements, in the same way as human reviewers. The comments made by static analyzers often trigger a conversation between developers to align on if and how the issue should be fixed. Because developers rarely give feedback directly to the tool, understanding the sentiment and intent in the conversation triggered by the tool comments can be used to measure the usefulness of the static analyzer.In this paper, we report on an experiment where we use large language models to automatically label and categorize the sentiment and intent of such conversations triggered by static analyzer comments. Our experiment demonstrates that LLMs not only classify and interpret complex developer-analyzer conversations, but can be more accurate than human experts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "1945\u20131955",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ]
    },
    "Ansible Lightspeed: A Code Generation Service for IT Automation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695277",
        "author": "Sahoo, Priyam and Pujar, Saurabh and Nalawade, Ganesh and Genhardt, Richard and Mandel, Louis and Buratti, Luca",
        "title": "Ansible Lightspeed: A Code Generation Service for IT Automation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695277",
        "doi": "10.1145/3691620.3695277",
        "abstract": "The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66\\% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50\\% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08\\% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2148\u20132158",
        "numpages": "11",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695290",
        "author": "Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang",
        "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695290",
        "doi": "10.1145/3691620.3695290",
        "abstract": "As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1\\%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2230\u20132234",
        "numpages": "5",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Attacks and Defenses for Large Language Models on Coding Tasks": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695297",
        "author": "Zhang, Chi and Wang, Zifan and Zhao, Ruoshi and Mangal, Ravi and Fredrikson, Matt and Jia, Limin and Pasareanu, Corina",
        "title": "Attacks and Defenses for Large Language Models on Coding Tasks",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695297",
        "doi": "10.1145/3691620.3695297",
        "abstract": "Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks, including writing and reasoning about code. They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities. However, these previous code models were shown vulnerable to adversarial examples, i.e., small syntactic perturbations designed to \"fool\" the models. In this paper, we first aim to study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs. We also propose a new attack using an LLM to generate the perturbations. Further, we propose novel cost-effective techniques to defend LLMs against such adversaries via prompting, without incurring the cost of retraining. These prompt-based defenses involve modifying the prompt to include additional information, such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations. Our preliminary experiments show the effectiveness of the attacks and the proposed defenses on popular LLMs such as GPT-3.5 and GPT-4.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2268\u20132272",
        "numpages": "5",
        "keywords": [
            "code model security",
            "attack",
            "defense"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695299",
        "author": "Peng, Chao and Wu, Qinyun and Liu, Jiangchao and Liu, Jierui and Jiang, Bo and Xu, Mengqian and Wang, Yinghao and Liu, Xia and Yang, Ping",
        "title": "RepoSim: Evaluating Prompt Strategies for Code Completion via User Behavior Simulation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695299",
        "doi": "10.1145/3691620.3695299",
        "abstract": "Large language models (LLMs) have revolutionized code completion tasks. IDE plugins such as MarsCode can generate code recommendations, saving developers significant time and effort. However, current evaluation methods for code completion are limited by their reliance on static code benchmarks, which do not consider human interactions and evolving repositories. This paper proposes RepoSim, a novel benchmark designed to evaluate code completion tasks by simulating the evolving process of repositories and incorporating user behaviors. RepoSim leverages data from an IDE plugin, by recording and replaying user behaviors to provide a realistic programming context for evaluation. This allows for the assessment of more complex prompt strategies, such as utilizing recently visited files and incorporating user editing history. Additionally, RepoSim proposes a new metric based on users' acceptance or rejection of predictions, offering a user-centric evaluation criterion. Our preliminary evaluation demonstrates that incorporating users' recent edit history into prompts significantly improves the quality of LLM-generated code, highlighting the importance of temporal context in code completion. RepoSim represents a significant advancement in benchmarking tools, offering a realistic and user-focused framework for evaluating code completion performance.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2279\u20132283",
        "numpages": "5",
        "keywords": [
            "code generation",
            "code completion"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "PACGBI: A Pipeline for Automated Code Generation from Backlog Items": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695346",
        "author": "Sarschar, Mahja and Zhang, Gefei and Nowak, Annika",
        "title": "PACGBI: A Pipeline for Automated Code Generation from Backlog Items",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695346",
        "doi": "10.1145/3691620.3695346",
        "abstract": "While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables nontechnical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2338\u20132341",
        "numpages": "4",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695349",
        "author": "Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong",
        "title": "ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695349",
        "doi": "10.1145/3691620.3695349",
        "abstract": "Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48\\%) are valid patches that fix the vulnerabilities, while 10 (21\\%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2350\u20132353",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "CoqPilot, a plugin for LLM-based generation of proofs": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695357",
        "author": "Kozyrev, Andrei and Solovev, Gleb and Khramov, Nikita and Podkopaev, Anton",
        "title": "CoqPilot, a plugin for LLM-based generation of proofs",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695357",
        "doi": "10.1145/3691620.3695357",
        "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2382\u20132385",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "LLM-Based Java Concurrent Program to ArkTS Converter": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695362",
        "author": "Liu, Runlin and Lin, Yuhang and Hu, Yunge and Zhang, Zhe and Gao, Xiang",
        "title": "LLM-Based Java Concurrent Program to ArkTS Converter",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695362",
        "doi": "10.1145/3691620.3695362",
        "abstract": "HarmonyOS NEXT is a distributed operating system developed to support HarmonyOS native apps. To support the new and independent Harmony ecosystem, developers are required to migrate their applications from Android to HarmonyOS. However, HarmonyOS utilizes ArkTS, a superset of TypeScript, as the programming language for application development. Hence, migrating applications to HarmonyOS requires translating programs across different program languages, e.g., Java, which is known to be very challenging, especially for concurrency programs. Java utilizes shared memory to implement concurrency programs, while ArkTS relies on message passing (i.e., Actor model). This paper presents an LLM-based concurrent Java program to ArkTS converter.Our converter utilizes large language models (LLMs) for efficient code translation, integrating ArkTS's SharedArrayBuffer API to create ThreadBridge, a library that replicates Java's shared memory model. Using LLM's Chain-of-Thought mechanism, the translation process is divided into specialized chains: the TS chain, concurrency chain, and synchronization chain, each handling TypeScript syntax, concurrency patterns, and synchronization logic with precision.This study offers solutions to bridge concurrency model differences between Java and ArkTS, reducing manual code rewriting and speeding up adaptation for HarmonyOS NEXT. Experiments show the converter successfully compiles 66\\% of 53 test samples, with 69\\% accuracy for compiled results. Overall, the approach shows promise in converting concurrent Java programs to ArkTS, laying the foundation for future improvements.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2403\u20132406",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Automated Validation of COBOL to Java Transformation": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695365",
        "author": "Kumar, Atul and Saha, Diptikalyan and Yasue, Toshiaki and Ono, Kohichi and Krishnan, Saravanan and Hans, Sandeep and Satoh, Fumiko and Mitchell, Gerald and Kumar, Sachin",
        "title": "Automated Validation of COBOL to Java Transformation",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695365",
        "doi": "10.1145/3691620.3695365",
        "abstract": "Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterpriselevel code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lU",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2415\u20132418",
        "numpages": "4",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Can Large Language Models Comprehend Code Stylometry?": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695370",
        "author": "Dipongkor, Atish",
        "title": "Can Large Language Models Comprehend Code Stylometry?",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695370",
        "doi": "10.1145/3691620.3695370",
        "abstract": "Code Authorship Attribution (CAA) has several applications such as copyright disputes, plagiarism detection and criminal prosecution. Existing studies mainly focused on CAA by proposing machine learning (ML) and Deep Learning (DL) based techniques. The main limitations of ML-based techniques are (a) manual feature engineering is required to train these models and (b) they are vulnerable to adversarial attack. In this study, we initially fine-tune five Large Language Models (LLMs) for CAA and evaluate their performance. Our results show that LLMs are robust and less vulnerable compared to existing techniques in CAA task.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2429\u20132431",
        "numpages": "3",
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024",
        "keywords": [
            "software maintenance and deployment",
            "supply chain"
        ]
    },
    "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695322",
        "author": "Luo, Yang and Yu, Richard and Zhang, Fajun and Liang, Ling and Xiong, Yongqiang",
        "title": "Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695322",
        "doi": "10.1145/3691620.3695322",
        "abstract": "When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2\\%.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2448\u20132449",
        "numpages": "2",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier": {
        "type": "inproceedings",
        "key": "10.1145/3691620.3695335",
        "author": "Moumoula, Micheline Benedicte and Kabore, Abdoul Kader and Klein, Jacques and Bissyande, Tegawende F.",
        "title": "Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier",
        "year": "2024",
        "isbn": "9798400712487",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3691620.3695335",
        "doi": "10.1145/3691620.3695335",
        "abstract": "Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem.This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pretrained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets.Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection.",
        "booktitle": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
        "pages": "2474\u20132475",
        "numpages": "2",
        "keywords": [
            "static analysis",
            "code clone detection"
        ],
        "location": "Sacramento, CA, USA",
        "series": "ASE '24",
        "venue": "ASE2024"
    },
    "Who Judges the Judge: An Empirical Study on Online Judge Tests": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598060",
        "author": "Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun",
        "title": "Who Judges the Judge: An Empirical Study on Online Judge Tests",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598060",
        "doi": "10.1145/3597926.3598060",
        "abstract": "Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4\\% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2\\% of false positives have perfect (100\\%) line coverage, 78.9\\% have perfect branch coverage, and 32.5\\% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "334\u2013346",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598067",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming",
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598067",
        "doi": "10.1145/3597926.3598067",
        "abstract": "Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\\%/50.84\\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "423\u2013435",
        "numpages": "13",
        "keywords": [
            "program testing",
            "fuzzing",
            "library testing"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "How Effective Are Neural Networks for Fixing Security Vulnerabilities": {
        "type": "inproceedings",
        "key": "10.1145/3597926.3598135",
        "author": "Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena",
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "year": "2023",
        "isbn": "9798400702211",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597926.3598135",
        "doi": "10.1145/3597926.3598135",
        "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\\%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs\u2019 vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.",
        "booktitle": "Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1282\u20131294",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "location": "Seattle, WA, USA",
        "series": "ISSTA 2023",
        "venue": "ISSTA2023"
    },
    "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00085",
        "author": "Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha",
        "title": "CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00085",
        "doi": "10.1109/ICSE48619.2023.00085",
        "abstract": "Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "919\u2013931",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "program testing",
            "fuzzing"
        ]
    },
    "CCTest: Testing and Repairing Code Completion Systems": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00110",
        "author": "Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun",
        "title": "CCTest: Testing and Repairing Code Completion Systems",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00110",
        "doi": "10.1109/ICSE48619.2023.00110",
        "abstract": "Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the \"average\" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86\\%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\\% and 67\\% with respect to BLEU score and Levenshtein edit similarity.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1238\u20131250",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "code completion"
        ]
    },
    "Automated Repair of Programs from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00128",
        "author": "Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei",
        "title": "Automated Repair of Programs from Large Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00128",
        "doi": "10.1109/ICSE48619.2023.00128",
        "abstract": "Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1469\u20131481",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Automated Program Repair in the Era of Large Pre-Trained Language Models": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00129",
        "author": "Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming",
        "title": "Automated Program Repair in the Era of Large Pre-Trained Language Models",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00129",
        "doi": "10.1109/ICSE48619.2023.00129",
        "abstract": "Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "1482\u20131494",
        "numpages": "13",
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction": {
        "type": "inproceedings",
        "key": "10.1109/ICSE48619.2023.00194",
        "author": "Kang, Sungmin and Yoon, Juyeon and Yoo, Shin",
        "title": "Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction",
        "year": "2023",
        "isbn": "9781665457019",
        "publisher": "IEEE Press",
        "url": "https://doi.org/10.1109/ICSE48619.2023.00194",
        "doi": "10.1109/ICSE48619.2023.00194",
        "abstract": "Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33\\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32\\% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",
        "booktitle": "Proceedings of the 45th International Conference on Software Engineering",
        "pages": "2312\u20132323",
        "numpages": "12",
        "keywords": [
            "program testing",
            "bug reproduction"
        ],
        "location": "Melbourne, Victoria, Australia",
        "series": "ICSE '23",
        "venue": "ICSE2023"
    },
    "COMEX: A Tool for Generating Customized Source Code Representations": {
        "type": "INPROCEEDINGS",
        "key": "10298568",
        "author": "Das, Debeshee and Mathews, Noble Saji and Mathai, Alex and Tamilselvam, Srikanth and Sedamaki, Kranthi and Chimalakonda, Sridhar and Kumar, Atul",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "COMEX: A Tool for Generating Customized Source Code Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "2054-2057",
        "abstract": "Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.",
        "keywords": [
            "code generation",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00010",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair": {
        "type": "INPROCEEDINGS",
        "key": "10298532",
        "author": "Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1162-1174",
        "abstract": "The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\\mathrm{C}/\\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00181",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "SMT Solver Validation Empowered by Large Pre-Trained Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298442",
        "author": "Sun, Maolin and Yang, Yibiao and Wang, Yang and Wen, Ming and Jia, Haoxiang and Zhou, Yuming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SMT Solver Validation Empowered by Large Pre-Trained Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1288-1300",
        "abstract": "SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "doi": "10.1109/ASE56229.2023.00180",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model": {
        "type": "INPROCEEDINGS",
        "key": "10298433",
        "author": "Malkadi, Abdulkarim and Tayeb, Ahmad and Haiduc, Sonia",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1492-1504",
        "abstract": "Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-the-art in code extraction techniques from screencasts and images.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "doi": "10.1109/ASE56229.2023.00184",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Better Patching Using LLM Prompting, via Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "10298561",
        "author": "Ahmed, Toufique and Devanbu, Premkumar",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Better Patching Using LLM Prompting, via Self-Consistency",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1742-1746",
        "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
        "keywords": [
            "code generation",
            "program repair",
            "prompting strategy"
        ],
        "doi": "10.1109/ASE56229.2023.00065",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT": {
        "type": "INPROCEEDINGS",
        "key": "10298505",
        "author": "Yan, Dapeng and Gao, Zhipeng and Liu, Zhiming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "1887-1898",
        "abstract": "Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of \u201ccode cleanness\u201d, we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "doi": "10.1109/ASE56229.2023.00096",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting": {
        "type": "INPROCEEDINGS",
        "key": "10298538",
        "author": "Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "14-26",
        "abstract": "Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences. We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.",
        "keywords": [
            "program testing",
            "differential testing"
        ],
        "doi": "10.1109/ASE56229.2023.00089",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices": {
        "type": "INPROCEEDINGS",
        "key": "10298463",
        "author": "Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "358-370",
        "abstract": "Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., \u201cProperly free allocated memory upon the completion of functions and at all exit points.\u201d, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73 % MLine on average, as well as coding explanations with 3.97 % F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.",
        "keywords": [
            "software maintenance and deployment",
            "code review"
        ],
        "doi": "10.1109/ASE56229.2023.00040",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "The Plastic Surgery Hypothesis in the Era of Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10298499",
        "author": "Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "The Plastic Surgery Hypothesis in the Era of Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "522-534",
        "abstract": "Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "doi": "10.1109/ASE56229.2023.00047",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?": {
        "type": "INPROCEEDINGS",
        "key": "10298329",
        "author": "Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "761-773",
        "abstract": "Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.",
        "keywords": [
            "general coding task"
        ],
        "doi": "10.1109/ASE56229.2023.00109",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining": {
        "type": "INPROCEEDINGS",
        "key": "10298349",
        "author": "Ren, Xiaoxue and Ye, Xinyuan and Zhao, Dehai and Xing, Zhenchang and Yang, Xiaohu",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "976-987",
        "abstract": "Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "doi": "10.1109/ASE56229.2023.00143",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Generative Type Inference for Python": {
        "type": "INPROCEEDINGS",
        "key": "10298512",
        "author": "Peng, Yun and Wang, Chaozheng and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.",
        "booktitle": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
        "title": "Generative Type Inference for Python",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "988-999",
        "abstract": "Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.",
        "keywords": [
            "static analysis",
            "type inference"
        ],
        "doi": "10.1109/ASE56229.2023.00031",
        "ISSN": "2643-1572",
        "month": "Sep.",
        "venue": "ASE2023"
    },
    "Lost at C: a user study on the security implications of large language model code assistants": {
        "type": "inproceedings",
        "key": "10.5555/3620237.3620361",
        "author": "Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan",
        "title": "Lost at C: a user study on the security implications of large language model code assistants",
        "year": "2023",
        "isbn": "978-1-939133-37-3",
        "publisher": "USENIX Association",
        "address": "USA",
        "abstract": "Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven user study (N=58) to assess code written by student programmers when assisted by LLMs. Given the potential severity of low-level bugs as well as their relative frequency in real-world projects, we tasked participants with implementing a singly-linked 'shopping list' structure in C. Our results indicate that the security impact in this setting (low-level C with pointer and array manipulations) is small: AI-assisted users produce critical security bugs at a rate no greater than 10\\% more than the control, indicating the use of LLMs does not introduce new security risks.",
        "booktitle": "Proceedings of the 32nd USENIX Conference on Security Symposium",
        "articleno": "124",
        "numpages": "18",
        "location": "Anaheim, CA, USA",
        "series": "SEC '23",
        "venue": "USENIXSec2023",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Examining Zero-Shot Vulnerability Repair with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "10179420",
        "author": "Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan",
        "booktitle": "2023 IEEE Symposium on Security and Privacy (SP)",
        "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
        "year": "2023",
        "volume": "",
        "ISSN": "",
        "pages": "2339-2356",
        "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
        "keywords": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "doi": "10.1109/SP46215.2023.10179420",
        "url": "https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179420",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "month": "May",
        "venue": "S&P2023"
    },
    "CodeT5+: Open Code Large Language Models for Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2023-codet5",
        "author": "Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven",
        "booktitle": "EMNLP2023",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \u201cCodeT5+\u201d, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "keywords": [
            "general coding task",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.68",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chen-etal-2023-personalized",
        "author": "Chen, Hailin and Saha, Amrita and Hoi, Steven and Joty, Shafiq",
        "booktitle": "EMNLP2023",
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.417",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Benchmarking and Improving Text-to-SQL Generation under Ambiguity": {
        "type": "INPROCEEDINGS",
        "key": "bhaskar-etal-2023-benchmarking",
        "author": "Bhaskar, Adithya and Tomar, Tushar and Sathe, Ashutosh and Sarawagi, Sunita",
        "booktitle": "EMNLP2023",
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.436",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Symbolic Planning and Code Generation for Grounded Dialogue": {
        "type": "INPROCEEDINGS",
        "key": "chiu-etal-2023-symbolic-planning",
        "author": "Chiu, Justin and Zhao, Wenting and Chen, Derek and Vaduguru, Saujas and Rush, Alexander and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.460",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Generating Data for Symbolic Language with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ye-etal-2023-generating",
        "author": "Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao",
        "booktitle": "EMNLP2023",
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "keywords": [
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.523",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs": {
        "type": "INPROCEEDINGS",
        "key": "aggarwal-etal-2023-lets",
        "author": "Aggarwal, Pranjal and Madaan, Aman and Yang, Yiming and Mausam",
        "booktitle": "EMNLP2023",
        "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "keywords": [
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.761",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Question Answering as Programming for Solving Time-Sensitive Questions": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2023-question",
        "author": "Zhu, Xinyu and Yang, Cheng and Chen, Bei and Li, Siheng and Lou, Jian-Guang and Yang, Yujiu",
        "booktitle": "EMNLP2023",
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs\u2019 inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs\u2019 superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.787",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL": {
        "type": "INPROCEEDINGS",
        "key": "kothyari-etal-2023-crush4sql",
        "author": "Kothyari, Mayank and Dhingra, Dhruva and Sarawagi, Sunita and Chakrabarti, Soumen",
        "booktitle": "EMNLP2023",
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination \u2014 generally considered a nuisance \u2014 turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.868",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "API-Assisted Code Generation for Question Answering on Varied Table Structures": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2023-api",
        "author": "Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP2023",
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures \u2014 relational, multi-table, and hierarchical matrix shapes \u2014 and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.897",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Prompting with Pseudo-Code Instructions": {
        "type": "INPROCEEDINGS",
        "key": "mishra-etal-2023-prompting",
        "author": "Mishra, Mayank and Kumar, Prince and Bhat, Riyaz and Murthy, Rudra and Contractor, Danish and Tamilselvam, Srikanth",
        "booktitle": "EMNLP2023",
        "title": "Prompting with Pseudo-Code Instructions",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.939",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Exploring Distributional Shifts in Large Language Models for Code Analysis": {
        "type": "INPROCEEDINGS",
        "key": "arakelyan-etal-2023-exploring",
        "author": "Arakelyan, Shushan and Das, Rocktim and Mao, Yi and Ren, Xiang",
        "booktitle": "EMNLP2023",
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "keywords": [
            "general coding task",
            "code model",
            "source code model",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-main.1013",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "MiniChain: A Small Library for Coding with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "rush-2023-minichain",
        "author": "Rush, Alexander",
        "booktitle": "EMNLP2023",
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.",
        "keywords": [
            "general coding task"
        ],
        "url": "https://doi.org/10.18653/v1/2023.emnlp-demo.27",
        "doi": "",
        "ISSN": "",
        "month": "December",
        "venue": "EMNLP2023"
    },
    "Self-Edit: Fault-Aware Code Editor for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-self",
        "author": "Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi",
        "booktitle": "ACL2023",
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.45",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Making Language Models Better Reasoners with Step-Aware Verifier": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "booktitle": "ACL2023",
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "keywords": [
            "prompt strategy",
            "sampling and ranking"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.291",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Fact-Checking Complex Claims with Program-Guided Reasoning": {
        "type": "INPROCEEDINGS",
        "key": "pan-etal-2023-fact",
        "author": "Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav",
        "booktitle": "ACL2023",
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "keywords": [
            "prompt strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.386",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Large Language Models Meet NL2Code: A Survey": {
        "type": "INPROCEEDINGS",
        "key": "zan-etal-2023-large",
        "author": "Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang",
        "booktitle": "ACL2023",
        "title": "Large Language Models Meet NL2Code: A Survey",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "keywords": [
            "survey",
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.411",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2023-codeie",
        "author": "Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng",
        "booktitle": "ACL2023",
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.855",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2023-xsemplr",
        "author": "Zhang, Yusen and Wang, Jun and Wang, Zhiguo and Zhang, Rui",
        "booktitle": "ACL2023",
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "year": "2023",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "keywords": [
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2023.acl-long.887",
        "doi": "",
        "ISSN": "",
        "month": "July",
        "venue": "ACL2023"
    },
    "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616271",
        "author": "Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616271",
        "doi": "10.1145/3611643.3616271",
        "abstract": "During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "172\u2013184",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Multilingual Code Co-evolution using Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616350",
        "author": "Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos",
        "title": "Multilingual Code Co-evolution using Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616350",
        "doi": "10.1145/3611643.3616350",
        "abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "695\u2013707",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation",
            "code model",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Baldur: Whole-Proof Generation and Repair with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616243",
        "author": "First, Emily and Rabe, Markus N. and Ringer, Talia and Brun, Yuriy",
        "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616243",
        "doi": "10.1145/3611643.3616243",
        "abstract": "Formally verifying software is a highly desirable but labor-intensive task.  Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.  This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.  We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.  This paper:  (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.  (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.  (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.  We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,  empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\\% of the theorems. Together, Baldur and Thor can prove 65.7\\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1229\u20131241",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "program verification"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Grace: Language Models Meet Code Edits": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3616253",
        "author": "Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish",
        "title": "Grace: Language Models Meet Code Edits",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3616253",
        "doi": "10.1145/3611643.3616253",
        "abstract": "Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29\\% and 54\\% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1483\u20131495",
        "numpages": "13",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "InferFix: End-to-End Program Repair with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613892",
        "author": "Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",
        "title": "InferFix: End-to-End Program Repair with LLMs",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613892",
        "doi": "10.1145/3611643.3613892",
        "abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose\u202f: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever \u2013 transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator \u2013 an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\\% for generating fixes in C# and 76.8\\% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "1646\u20131656",
        "numpages": "11",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3613078",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Assisting Static Analysis with Large Language Models: A ChatGPT Experiment",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3613078",
        "doi": "10.1145/3611643.3613078",
        "abstract": "Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2107\u20132111",
        "numpages": "5",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "LLM-Based Code Generation Method for Golang Compiler Testing": {
        "type": "inproceedings",
        "key": "10.1145/3611643.3617850",
        "author": "Gu, Qiuhan",
        "title": "LLM-Based Code Generation Method for Golang Compiler Testing",
        "year": "2023",
        "isbn": "9798400703270",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3611643.3617850",
        "doi": "10.1145/3611643.3617850",
        "abstract": "Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\\%. Moreover, among all the generated testcases, only 2.79\\% exhibited syntax errors, and none displayed undefined behavior.",
        "booktitle": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "pages": "2201\u20132203",
        "numpages": "3",
        "keywords": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "location": "San Francisco, CA, USA",
        "series": "ESEC/FSE 2023",
        "venue": "FSE2023"
    },
    "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities": {
        "type": "article",
        "key": "10.1145/3664606",
        "author": "Ma, Wei and Liu, Shangqing and Zhao, Mengjie and Xie, Xiaofei and Wang, Wenhang and Hu, Qiang and Zhang, Jie and Liu, Yang",
        "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664606",
        "doi": "10.1145/3664606",
        "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree&nbsp;(AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models\u2019 capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models\u2019 abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "169",
        "numpages": "29",
        "keywords": [
            "static analysis",
            "foundamental analysis",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Planning Code Generation with Large Language Models": {
        "type": "article",
        "key": "10.1145/3672456",
        "author": "Jiang, Xue and Dong, Yihong and Wang, Lecheng and Fang, Zheng and Shang, Qiwei and Li, Ge and Jin, Zhi and Jiao, Wenpin",
        "title": "Self-Planning Code Generation with Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672456",
        "doi": "10.1145/3672456",
        "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\\% in Pass@1 compared to direct code generation, and up to 11.9\\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "182",
        "numpages": "30",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code": {
        "type": "article",
        "key": "10.1145/3672458",
        "author": "Huang, Qing and Luo, Zhiwen and Xing, Zhenchang and Zeng, Jinshan and Chen, Jieshan and Xu, Xiwei and Chen, Yong",
        "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Dataflows to Generate Dataflow Graphs in Dynamically Typed Code",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672458",
        "doi": "10.1145/3672458",
        "abstract": "Dataflow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically typed programming languages like Python present implicit dataflow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit dataflow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs\u2019 in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit dataflow problems. To further enhance the accuracy of LLMs, we design a five-step chain of thought (CoT) and break it down into an Artificial Intelligence (AI) chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach\u2019s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82\\% higher def coverage and 58\\% higher use coverage in DFG generation on implicit dataflow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "183",
        "numpages": "29",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "venue": "TOSEM2024"
    },
    "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models": {
        "type": "article",
        "key": "10.1145/3664812",
        "author": "Feng, Xiaoning and Han, Xiaohong and Chen, Simin and Yang, Wei",
        "title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of Large Language Models",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3664812",
        "doi": "10.1145/3664812",
        "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker, which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker, we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs\u2019 response latency and energy consumption by 325\\% to 3,244\\% and 344\\% to 3,616\\%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "aug",
        "articleno": "186",
        "numpages": "38",
        "keywords": [
            "code model security",
            "attack"
        ],
        "venue": "TOSEM2024"
    },
    "Self-Collaboration Code Generation via ChatGPT": {
        "type": "article",
        "key": "10.1145/3672459",
        "author": "Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge",
        "title": "Self-Collaboration Code Generation via ChatGPT",
        "year": "2024",
        "issue_date": "September 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "7",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3672459",
        "doi": "10.1145/3672459",
        "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct \u201cexperts,\u201d each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other\u2019s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development\u2019s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\u201347.1\\% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "sep",
        "articleno": "189",
        "numpages": "38",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "venue": "TOSEM2024"
    },
    "Risky Dynamic Typing-related Practices in Python: An Empirical Study": {
        "type": "article",
        "key": "10.1145/3649593",
        "author": "Chen, Zhifei and Chen, Lin and Yang, Yibiao and Feng, Qiong and Li, Xuansong and Song, Wei",
        "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "33",
        "number": "6",
        "issn": "1049-331X",
        "url": "https://doi.org/10.1145/3649593",
        "doi": "10.1145/3649593",
        "abstract": "Python\u2019s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers\u2019 high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models\u2013based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "month": "jun",
        "articleno": "140",
        "numpages": "35",
        "keywords": [
            "static analysis",
            "type inference",
            "bug detection",
            "empirical study"
        ],
        "venue": "TOSEM2024"
    },
    "Statically Contextualizing Large Language Models with Typed Holes": {
        "type": "article",
        "key": "10.1145/3689728",
        "author": "Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus",
        "title": "Statically Contextualizing Large Language Models with Typed Holes",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689728",
        "doi": "10.1145/3689728",
        "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "288",
        "numpages": "31",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark",
            "empirical study"
        ],
        "venue": "OOPSLA2024"
    },
    "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs": {
        "type": "article",
        "key": "10.1145/3689735",
        "author": "Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun",
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689735",
        "doi": "10.1145/3689735",
        "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).                                                                                                                                                                                                                                                              This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.                                                                                                                                                                                                                                                              Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "295",
        "numpages": "32",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "venue": "OOPSLA2024"
    },
    "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models": {
        "type": "article",
        "key": "10.1145/3689736",
        "author": "Yang, Chenyuan and Deng, Yinlin and Lu, Runyu and Yao, Jiayi and Liu, Jiawei and Jabbarvand, Reyhaneh and Zhang, Lingming",
        "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689736",
        "doi": "10.1145/3689736",
        "abstract": "Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.              To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "296",
        "numpages": "27",
        "keywords": [
            "program testing",
            "fuzzing",
            "code model",
            "source code model"
        ],
        "venue": "OOPSLA2024"
    },
    "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models": {
        "type": "article",
        "key": "10.1145/3689776",
        "author": "Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu",
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "year": "2024",
        "issue_date": "October 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA2",
        "url": "https://doi.org/10.1145/3689776",
        "doi": "10.1145/3689776",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations.    To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth.    Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7\\% to 59.8\\%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.    To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "oct",
        "articleno": "336",
        "numpages": "30",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "venue": "OOPSLA2024"
    },
    "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach": {
        "type": "article",
        "key": "10.1145/3649828",
        "author": "Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun",
        "title": "Enhancing Static Analysis for Practical Bug Detection: An LLM-Integrated Approach",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649828",
        "doi": "10.1145/3649828",
        "abstract": "While static analysis is instrumental in uncovering software bugs, its precision in analyzing large and intricate codebases remains challenging. The emerging prowess of Large Language Models (LLMs) offers a promising avenue to address these complexities. In this paper, we present LLift, a pioneering framework that synergizes static analysis and LLMs, with a spotlight on identifying use-before-initialization (UBI) bugs within the Linux kernel. Drawing from our insights into variable usage conventions in Linux, we enhance path analysis using post-constraint guidance. This approach, combined with our methodically crafted procedures, empowers LLift to adeptly handle the challenges of bug-specific modeling, extensive codebases, and the unpredictable nature of LLMs. Our real-world evaluations identified four previously undiscovered UBI bugs in the mainstream Linux kernel, which the Linux community has acknowledged. This study reaffirms the potential of marrying static analysis with LLMs, setting a compelling direction for future research in this area.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "111",
        "numpages": "26",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "venue": "OOPSLA2024"
    },
    "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs": {
        "type": "article",
        "key": "10.1145/3649850",
        "author": "Zhang, Jialu and Cambronero, Jos\\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust",
        "title": "PyDex: Repairing Bugs in Introductory Python Assignments using LLMs",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "8",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3649850",
        "doi": "10.1145/3649850",
        "abstract": "Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "133",
        "numpages": "25",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "venue": "OOPSLA2024"
    },
    "TransLLaMa: LLM-based Simultaneous Translation System": {
        "type": "INPROCEEDINGS",
        "key": "koshkin-etal-2024-transllama",
        "author": "Koshkin, Roman and Sudoh, Katsuhito and Nakamura, Satoshi",
        "booktitle": "EMNLP-findings2024",
        "title": "TransLLaMa: LLM-based Simultaneous Translation System",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \u201cwait\u201d token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
        "keywords": [
            "code generation",
            "program transformation",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly": {
        "type": "INPROCEEDINGS",
        "key": "zhang-etal-2024-introducing",
        "author": "Zhang, Shuoming and Zhao, Jiacheng and Xia, Chunwei and Wang, Zheng and Chen, Yunji and Cui, Huimin",
        "booktitle": "EMNLP-findings2024",
        "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91%, outperforming the much larger GPT-4 Turbo model by over 50%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed.",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.55",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "EvoR: Evolving Retrieval for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "su-etal-2024-evor",
        "author": "Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao",
        "booktitle": "EMNLP-findings2024",
        "title": "EvoR: Evolving Retrieval for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research.",
        "keywords": [
            "code generation",
            "code completion",
            "source code model",
            "prompting strategy",
            "RAG"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.143",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization": {
        "type": "INPROCEEDINGS",
        "key": "nahid-rafiei-2024-normtab",
        "author": "Nahid, Md and Rafiei, Davood",
        "booktitle": "EMNLP-findings2024",
        "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.",
        "keywords": [
            "static analysis",
            "fundamental analysis"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.203",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Sanitizing Large Language Models in Bug Detection with Data-Flow": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-sanitizing",
        "author": "Wang, Chengpeng and Zhang, Wuqi and Su, Zian and Xu, Xiangzhe and Zhang, Xiangyu",
        "booktitle": "EMNLP-findings2024",
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition. Specifically, we dissect data-flow paths into basic properties upon concise code snippets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively.",
        "keywords": [
            "static analysis",
            "bug detection",
            "fundamental analysis"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.217",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Stanceformer: Target-Aware Transformer for Stance Detection": {
        "type": "INPROCEEDINGS",
        "key": "garg-caragea-2024-stanceformer",
        "author": "Garg, Krishna and Caragea, Cornelia",
        "booktitle": "EMNLP-findings2024",
        "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task\u2019s significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a Target Awareness matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.",
        "keywords": [
            "static analysis",
            "bug detection",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.286",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing": {
        "type": "INPROCEEDINGS",
        "key": "zhao-etal-2024-defending-large",
        "author": "Zhao, Wei and Li, Zhe and Li, Yige and Zhang, Ye and Sun, Jun",
        "booktitle": "EMNLP-findings2024",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed Layer-specific Editing (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical safety layers exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified toxic layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at https://github.com/ledllm/ledllm.",
        "keywords": [
            "code model security",
            "defense"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.293",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement": {
        "type": "INPROCEEDINGS",
        "key": "feng-etal-2024-self",
        "author": "Feng, Yunlong and Teng, Dechuan and Xu, Yang and Mu, Honglin and Xu, Xiao and Qin, Libo and Zhu, Qingfu and Che, Wanxiang",
        "booktitle": "EMNLP-findings2024",
        "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc\u00b2dec) method recompiles the LLM\u2019s decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec.",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model",
            "source code model",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.385",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "cheng-etal-2024-autodetect",
        "author": "Cheng, Jiale and Lu, Yida and Gu, Xiaotao and Ke, Pei and Liu, Xiao and Dong, Yuxiao and Wang, Hongning and Tang, Jie and Huang, Minlie",
        "booktitle": "EMNLP-findings2024",
        "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks.As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically.Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students\u2019 learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor.The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude.More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks.Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
        "keywords": [
            "code model security",
            "attack",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.397",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code": {
        "type": "INPROCEEDINGS",
        "key": "guan-etal-2024-codeip",
        "author": "Guan, Batu and Wan, Yao and Bi, Zhangqian and Wang, Zheng and Zhang, Hongyu and Zhou, Pan and Sun, Lichao",
        "booktitle": "EMNLP-findings2024",
        "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that embeds additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model security",
            "defense"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.541",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging": {
        "type": "INPROCEEDINGS",
        "key": "kargupta-etal-2024-instruct",
        "author": "Kargupta, Priyanka and Agarwal, Ishika and Tur, Dilek Hakkani and Han, Jiawei",
        "booktitle": "EMNLP-findings2024",
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student\u2019s conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes\u2013 all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct\u2019s state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct\u2019s ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.",
        "keywords": [
            "program testing",
            "debugging",
            "agent design",
            "planning"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.553",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Can LLMs Reason in the Wild with Programs?": {
        "type": "INPROCEEDINGS",
        "key": "yang-etal-2024-llms",
        "author": "Yang, Yuan and Xiong, Siheng and Payani, Ali and Shareghi, Ehsan and Fekri, Faramarz",
        "booktitle": "EMNLP-findings2024",
        "title": "Can LLMs Reason in the Wild with Programs?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown superior capability to solve reasoning problems with programs. While being a promising direction, most of such frameworks are trained and evaluated in settings with a prior knowledge of task requirements. However, as LLMs become more capable, it is necessary to assess their reasoning abilities in more realistic scenarios where many real-world problems are open-ended with ambiguous scope, and often require multiple formalisms to solve. To investigate this, we introduce the task of reasoning in the wild, where an LLM is tasked to solve a reasoning problem of unknown type by identifying the sub-problems and their corresponding formalisms, and writing a program to solve each sub-problem, guided by a tactic. We create a large tactic-guided trajectory dataset containing detailed solutions to a diverse set of reasoning problems, ranging from well-defined single-form reasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense, combined math and logic). This allows us to test various aspects of LLMs reasoning at the fine-grained level such as the selection and execution of tactics, and the tendency to take undesired shortcuts. In experiments, we highlight that existing LLMs fail significantly on problems with ambiguous and mixed scope, revealing critical limitations and overfitting issues (e.g. accuracy on GSM8K drops by at least 50%). We further show the potential of finetuning a local LLM on the tactic-guided trajectories in achieving better performance. Project repo is available at https://github.com/gblackout/Reason-in-the-Wild.",
        "keywords": [
            "prompting strategy",
            "reasoning with code"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.573",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Rethinking Code Refinement: Learning to Judge Code Efficiency": {
        "type": "INPROCEEDINGS",
        "key": "seo-etal-2024-rethinking",
        "author": "Seo, Minju and Baek, Jinheon and Hwang, Sung Ju",
        "booktitle": "EMNLP-findings2024",
        "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.",
        "keywords": [
            "code generation",
            "program transformation",
            "code model",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.645",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Revisiting the Impact of Pursuing Modularity for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "kang-etal-2024-revisiting",
        "author": "Kang, Deokyeong and Seo, KiJung and Kim, Taeuk",
        "booktitle": "EMNLP-findings2024",
        "title": "Revisiting the Impact of Pursuing Modularity for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.676",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation": {
        "type": "INPROCEEDINGS",
        "key": "shen-etal-2024-enhancing",
        "author": "Shen, Zizhuo and Shao, Yanqiu and Li, Wei",
        "booktitle": "EMNLP-findings2024",
        "title": "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Due to the high complexity of Discourse Dependency Parsing (DDP) tasks, their existing annotation resources are relatively scarce compared to other NLP tasks, and different DDP tasks also have significant differences in annotation schema. These issues have led to the dilemma of low resources for DDP tasks. Thanks to the powerful capabilities of Large Language Models (LLMs) in cross-task learning, we can use LLMs to model dependency parsing under different annotation schema in an unified manner, in order to alleviate the dilemma of low resources for DDP tasks. However, enabling LLMs to deeply comprehend dependency parsing tasks is a challenge that remains underexplored. Inspired by the application of code-based methods in complex tasks, we propose a code-based unified dependency parsing method. We treat the process of dependency parsing as a search process of dependency paths and use code to represent this search process. Furthermore, we use a curriculum-learning based instruction tuning strategy for joint training of multiple dependency parsing tasks. The experimental results show that our proposed code-based DDP system has achieved good performance on two Chinese DDP tasks (especially significant improvement on the DDP task with relatively less training data).",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.729",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "On Leakage of Code Generation Evaluation Datasets": {
        "type": "INPROCEEDINGS",
        "key": "matton-etal-2024-leakage",
        "author": "Matton, Alexandre and Sherborne, Tom and Aumiller, Dennis and Tommasone, Elena and Alizadeh, Milad and He, Jingyi and Ma, Raymond and Voisin, Maxime and Gilsenan-McMahon, Ellen and Gall\u00e9, Matthias",
        "booktitle": "EMNLP-findings2024",
        "title": "On Leakage of Code Generation Evaluation Datasets",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In this paper, we consider contamination by code generation test sets, in particular in their use in modern large language models.We discuss three possible sources of such contamination and show findings supporting each of them: (i) direct data leakage, (ii) indirect data leakage through the use of synthetic data and (iii) overfitting to evaluation sets during model selection.To address this, we release Less Basic Python Problems (LBPP): an uncontaminated new benchmark of 161 prompts with their associated Python solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.772",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation": {
        "type": "article",
        "key": "10.1145/3643774",
        "author": "Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.",
        "title": "AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643774",
        "doi": "10.1145/3643774",
        "abstract": "Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges.        To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40\\% and 58\\% of the time, an improvement of 1.4\\texttimes{} and 4.1\\texttimes{} over a model trained only on public data.        We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8\\% of their code coming directly from CodeCompose.        To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5\\% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "48",
        "numpages": "20",
        "keywords": [
            "code generation",
            "code model",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models": {
        "type": "article",
        "key": "10.1145/3643776",
        "author": "Zhang, Zejun and Xing, Zhenchang and Ren, Xiaoxue and Lu, Qinghua and Xu, Xiwei",
        "title": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643776",
        "doi": "10.1145/3643776",
        "abstract": "Pythonic idioms are highly valued and widely used in the Python programming community. However, many  Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90\\% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90\\% for accuracy, F1-score, precision, and recall.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "50",
        "numpages": "22",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "venue": "FSE2024"
    },
    "Can GPT-4 Replicate Empirical Software Engineering Research?": {
        "type": "article",
        "key": "10.1145/3660767",
        "author": "Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas",
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660767",
        "doi": "10.1145/3660767",
        "abstract": "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.  In this paper, we examine GPT-4\u2019s abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "60",
        "numpages": "24",
        "keywords": [
            "empirical study",
            "agent design"
        ],
        "venue": "FSE2024"
    },
    "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach": {
        "type": "article",
        "key": "10.1145/3660769",
        "author": "Jin, Xin and Lin, Zhiqiang",
        "title": "SimLLM: Calculating Semantic Similarity in Code Summaries using a Large Language Model-Based Approach",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660769",
        "doi": "10.1145/3660769",
        "abstract": "Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "62",
        "numpages": "24",
        "keywords": [
            "static analysis",
            "code summarization"
        ],
        "venue": "FSE2024"
    },
    "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization": {
        "type": "article",
        "key": "10.1145/3660771",
        "author": "Kang, Sungmin and An, Gabin and Yoo, Shin",
        "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660771",
        "doi": "10.1145/3660771",
        "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3\\% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "64",
        "numpages": "23",
        "keywords": [
            "program testing",
            "debugging"
        ],
        "venue": "FSE2024"
    },
    "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation": {
        "type": "article",
        "key": "10.1145/3660778",
        "author": "Yang, Zhen and Liu, Fang and Yu, Zhongxing and Keung, Jacky Wai and Li, Jia and Liu, Shuo and Hong, Yifan and Ma, Xiaoxue and Jin, Zhi and Li, Ge",
        "title": "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660778",
        "doi": "10.1145/3660778",
        "abstract": "Code translation tools, namely transpilers, are developed for automatic source-to-source translation. Latest learning-based transpilers have shown impressive enhancement against rule-based counterparts in both translation accuracy and readability, owing to their task-specific pre-training on extensive monolingual corpora. Nevertheless, their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. Large Language Models (LLMs), pre-trained on huge amounts of human-written code/text, have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific re-training/fine-tuning. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51\\%), missing clear instructions on I/O types in translation (14.94\\%), and ignoring discrepancies between source and target programs (41.38\\%).  Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes, including GPT-3.5 and LLaMA-13B/7B, are tested with UniTrans, and all achieve substantial improvements in terms of computational accuracy and exact match accuracy among almost all translation settings, showing the universal effectiveness of UniTrans in practice.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "71",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program transformation",
            "code model",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Evaluating and Improving ChatGPT for Unit Test Generation": {
        "type": "article",
        "key": "10.1145/3660783",
        "author": "Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling",
        "title": "Evaluating and Improving ChatGPT for Unit Test Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660783",
        "doi": "10.1145/3660783",
        "abstract": "Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code.                                                                                                In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved.                                                                                                Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3\\% more compilable tests and 18.7\\% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLLama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "76",
        "numpages": "24",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study",
            "code generation"
        ],
        "venue": "FSE2024"
    },
    "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice": {
        "type": "article",
        "key": "10.1145/3660788",
        "author": "Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes",
        "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660788",
        "doi": "10.1145/3660788",
        "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "81",
        "numpages": "22",
        "keywords": [
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?": {
        "type": "article",
        "key": "10.1145/3660791",
        "author": "Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660791",
        "doi": "10.1145/3660791",
        "abstract": "Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program\u2019s intent. However, there is typically no guarantee that a program\u2019s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The \u201cemergent abilities\u201d of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "84",
        "numpages": "24",
        "keywords": [
            "static analysis",
            "specification inference",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice": {
        "type": "article",
        "key": "10.1145/3660806",
        "author": "Olewicki, Doriane and Habchi, Sarra and Adams, Bram",
        "title": "An Empirical Study on Code Review Activity Prediction and Its Impact in Practice",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660806",
        "doi": "10.1145/3660806",
        "abstract": "During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author\u2019s and the reviewer\u2019s experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23\\%), and participants\u2019 file-level hot-spot precision and recall increases to 53\\% (+13\\%) and 28\\% (+8\\%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62\\%) are significantly better than the state-of-the-art (from +1 to +9\\%).",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "99",
        "numpages": "23",
        "keywords": [
            "software maintenance and deployment",
            "code review",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?": {
        "type": "article",
        "key": "10.1145/3660807",
        "author": "Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi",
        "title": "Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660807",
        "doi": "10.1145/3660807",
        "abstract": "Large Language Models (LLMs) have recently been widely used for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation. An analysis of six LLMs, including GPT-4, on two popular code generation benchmarks revealed a consistent misalignment between LLMs' and programmers' attention. We manually analyzed 211 incorrect code snippets and found five attention patterns that can be used to explain many code generation errors. Finally, a user study showed that model attention computed by a perturbation-based method is often favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "100",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "Mining Action Rules for Defect Reduction Planning": {
        "type": "article",
        "key": "10.1145/3660809",
        "author": "Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse",
        "title": "Mining Action Rules for Defect Reduction Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660809",
        "doi": "10.1145/3660809",
        "abstract": "Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and \u201cexplaining\u201d its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT\u2019s explainable plans achieve higher overlap scores at the release level (median 95\\%) and commit level (median 85.97\\%), and they offer better trade-off between precision and recall (median F1-score 88.12\\%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "102",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "venue": "FSE2024"
    },
    "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification": {
        "type": "article",
        "key": "10.1145/3660810",
        "author": "Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing",
        "title": "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660810",
        "doi": "10.1145/3660810",
        "abstract": "Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\\% to 80.80\\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\\% to 69.60\\% and from 54.32\\% to 62.37\\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "103",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "venue": "FSE2024"
    },
    "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning": {
        "type": "article",
        "key": "10.1145/3660811",
        "author": "Mai, Yubo and Gao, Zhipeng and Hu, Xing and Bao, Lingfeng and Liu, Yu and Sun, JianLing",
        "title": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3660811",
        "doi": "10.1145/3660811",
        "abstract": "Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65\\%) and return statements (66\\%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0\\% and 16.5\\% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "104",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program synthesis",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-unleashing",
        "author": "Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng",
        "booktitle": "NAACL2024",
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "keywords": [
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.15",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Program-Aided Reasoners (Better) Know What They Know": {
        "type": "INPROCEEDINGS",
        "key": "kabra-etal-2024-program",
        "author": "Kabra, Anubha and Rangreji, Sanketh and Mathur, Yash and Madaan, Aman and Liu, Emmy and Neubig, Graham",
        "booktitle": "NAACL2024",
        "title": "Program-Aided Reasoners (Better) Know What They Know",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \u201cknow what they know\u201d, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.125",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning": {
        "type": "INPROCEEDINGS",
        "key": "zhu-etal-2024-pad",
        "author": "Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen",
        "booktitle": "NAACL2024",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.142",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Evaluating In-Context Learning of Libraries for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "patel-etal-2024-evaluating",
        "author": "Patel, Arkil and Reddy, Siva and Bahdanau, Dzmitry and Dasigi, Pradeep",
        "booktitle": "NAACL2024",
        "title": "Evaluating In-Context Learning of Libraries for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.161",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback": {
        "type": "INPROCEEDINGS",
        "key": "wu-2024-uicoder",
        "author": "Wu, Jason and Schoop, Eldon and Leung, Alan and Barik, Titus and Bigham, Jeffrey and Nichols, Jeffrey",
        "booktitle": "NAACL2024",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.naacl-long.417",
        "doi": "",
        "ISSN": "",
        "month": "June",
        "venue": "NAACL2024"
    },
    "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation": {
        "type": "INPROCEEDINGS",
        "key": "salim-etal-2024-impeding",
        "author": "Salim, Saiful Islam and Yang, Rubin Yuchan and Cooper, Alexander and Ray, Suryashree and Debray, Saumya and Rahaman, Sazzadur",
        "booktitle": "EMNLP-main2024",
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.27",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-amr",
        "author": "Luo, Ziyang and Li, Xin and Lin, Hongzhan and Ma, Jing and Bing, Lidong",
        "booktitle": "EMNLP-main2024",
        "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks\u2014HumanEval, MBPP, and EvalPlus\u2014attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.66",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "LLM4Decompile: Decompiling Binary Code with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tan-etal-2024-llm4decompile",
        "author": "Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun",
        "booktitle": "EMNLP-main2024",
        "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",
        "keywords": [
            "code generation",
            "program decompilation",
            "code model",
            "binary code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.203",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL": {
        "type": "INPROCEEDINGS",
        "key": "luo-etal-2024-ptd",
        "author": "Luo, Ruilin and Wang, Liyuan and Lin, Binghuai and Lin, Zicheng and Yang, Yujiu",
        "booktitle": "EMNLP-main2024",
        "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.221",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "How Do Humans Write Code? Large Models Do It the Same Way Too": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-humans",
        "author": "Li, Long and He, Xuzheng and Wang, Haozhe and Wang, Linlin and He, Liang",
        "booktitle": "EMNLP-main2024",
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model\u2019s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.267",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "chidambaram-etal-2024-socratic",
        "author": "Chidambaram, Subramanian and Li, Li Erran and Bai, Min and Li, Xiaopeng and Lin, Kaixiang and Zhou, Xiong and Williams, Alex C.",
        "booktitle": "EMNLP-findings2024",
        "title": "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) are increasingly used for generating code solutions, empowered by features like self-debugging and self-reflection. However, LLMs often struggle with complex programming problems without human guidance. This paper investigates the strategies employed by expert programmers to steer code-generating LLMs toward successful outcomes. Through a study involving experts using natural language to guide GPT-4, Gemini Ultra, and, Claude 3.5 Sonnet on highly difficult programming challenges, we frame our analysis using the \u201cSocratic Feedback\u201d paradigm for understanding effective steering strategies. By analyzing 30 conversational transcripts across all three models, we map observed feedback strategies to five stages of Socratic Questioning: Definition, Elenhus, Maieutic, Dialectic, and Counter-factual reasoning. We find evidence that by employing a combination of different Socratic feedback strategies across multiple turns, programmers successfully guided the models to solve 74% of the problems that the models initially failed to solve on their own.",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.908",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs": {
        "type": "INPROCEEDINGS",
        "key": "yadav-etal-2024-pythonsaga",
        "author": "Yadav, Ankit and Beniwal, Himanshu and Singh, Mayank",
        "booktitle": "EMNLP-findings2024",
        "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of *HumanEval* and *MBPP*, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks that can inflate model performance estimations. To address these limitations, we propose a novel benchmark, *PythonSaga*, featuring 185 hand-crafted prompts in a balanced representation of 38 programming concepts across diverse difficulty levels. The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs. The code and data set are openly available to the NLP community at this [URL](https://github.com/PythonSaga/PythonSaga).",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.findings-emnlp.996",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-findings2024"
    },
    "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428324",
        "author": "Nashaat, Mona and Miller, James",
        "title": "Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428324",
        "doi": "10.1109/TSE.2024.3428324",
        "abstract": "Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2240\u20132253",
        "numpages": "14",
        "venue": "TSE2024",
        "keywords": [
            "code generation",
            "program repair",
            "code model",
            "source code model"
        ]
    },
    "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3428972",
        "author": "Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.",
        "title": "LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3428972",
        "doi": "10.1109/TSE.2024.3428972",
        "abstract": "Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow &lt;sc&gt;TiCoder&lt;/sc&gt; for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97\\% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jul",
        "pages": "2254\u20132268",
        "numpages": "15",
        "venue": "TSE2024",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3440503",
        "author": "Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue",
        "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
        "year": "2024",
        "issue_date": "Sept. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "9",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3440503",
        "doi": "10.1109/TSE.2024.3440503",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (&lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq1-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq2-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach &lt;monospace&gt;COTTON&lt;/monospace&gt; which can leverage &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq3-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by &lt;monospace&gt;COTTON&lt;/monospace&gt; boost various &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq4-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that &lt;monospace&gt;COTTON&lt;/monospace&gt; not only improves the performance of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq5-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs, but also enhances the performance of LLMs. Our study showcases the potential of &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$ell$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;\u2113&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"zhou-ieq6-3440503.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;LMs in software engineering applications.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2437\u20132457",
        "numpages": "21",
        "venue": "TSE2024",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models": {
        "type": "article",
        "key": "10.1109/TSE.2024.3397822",
        "author": "Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao",
        "title": "Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "7",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3397822",
        "doi": "10.1109/TSE.2024.3397822",
        "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named &lt;sc&gt;LLM4CBI&lt;/sc&gt; to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in &lt;sc&gt;LLM4CBI&lt;/sc&gt;. First, &lt;sc&gt;LLM4CBI&lt;/sc&gt; utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, &lt;sc&gt;LLM4CBI&lt;/sc&gt; employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of &lt;sc&gt;LLM4CBI&lt;/sc&gt;: It can isolate 69.70\\%/21.74\\% and 24.44\\%/8.92\\% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in &lt;sc&gt;LLM4CBI&lt;/sc&gt; can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "may",
        "pages": "1768\u20131788",
        "numpages": "21",
        "venue": "TSE2024",
        "keywords": [
            "program testing",
            "debugging",
            "code model",
            "source code model"
        ]
    },
    "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation": {
        "type": "article",
        "key": "10.1109/TSE.2024.3382365",
        "author": "Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu",
        "title": "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3382365",
        "doi": "10.1109/TSE.2024.3382365",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "mar",
        "pages": "1340\u20131359",
        "numpages": "20",
        "venue": "TSE2024",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study"
        ]
    },
    "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT": {
        "type": "article",
        "key": "10.1109/TSE.2024.3392499",
        "author": "Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT",
        "year": "2024",
        "issue_date": "June 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "6",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3392499",
        "doi": "10.1109/TSE.2024.3392499",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using &lt;italic&gt;ChatGPT&lt;/italic&gt;, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by &lt;italic&gt;ChatGPT&lt;/italic&gt;, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to engage in multi-round fixing process (i.e., &lt;italic&gt;ChatGPT&lt;/italic&gt;'s dialog ability, chatting between users and &lt;italic&gt;ChatGPT&lt;/italic&gt; for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of &lt;italic&gt;ChatGPT&lt;/italic&gt; in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) &lt;italic&gt;ChatGPT&lt;/italic&gt; is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$48.14\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;48.14&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq1-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; advantage in &lt;italic&gt;Accepted&lt;/italic&gt; rate on judgment platform, but &lt;italic&gt;ChatGPT&lt;/italic&gt;'s ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with &lt;italic&gt;ChatGPT &lt;/italic&gt; generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by &lt;italic&gt;ChatGPT &lt;/italic&gt; has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$89\\%$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mn&gt;89&lt;/mml:mn&gt;&lt;mml:mi mathvariant=\"normal\"&gt;\\%&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"tang-ieq2-3392499.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; of vulnerabilities successfully addressed; and (4) code generation may be affected by &lt;italic&gt;ChatGPT&lt;/italic&gt;'s non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the &lt;italic&gt;ChatGPT&lt;/italic&gt;-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "apr",
        "pages": "1548\u20131584",
        "numpages": "37",
        "venue": "TSE2024",
        "keywords": [
            "code generation",
            "program synthesis",
            "empirical study"
        ]
    },
    "Automatic Commit Message Generation: A Critical Review and Directions for Future Work": {
        "type": "article",
        "key": "10.1109/TSE.2024.3364675",
        "author": "Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui",
        "title": "Automatic Commit Message Generation: A Critical Review and Directions for Future Work",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3364675",
        "doi": "10.1109/TSE.2024.3364675",
        "abstract": "Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of \u2018noise\u2019; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models \u2018learn\u2019 inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "816\u2013835",
        "numpages": "20",
        "venue": "TSE2024",
        "keywords": [
            "software maintenance and deployment",
            "commit message generation",
            "empirical study"
        ]
    },
    "Software Testing With Large Language Models: Survey, Landscape, and Vision": {
        "type": "article",
        "key": "10.1109/TSE.2024.3368208",
        "author": "Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
        "year": "2024",
        "issue_date": "April 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "4",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3368208",
        "doi": "10.1109/TSE.2024.3368208",
        "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "feb",
        "pages": "911\u2013936",
        "numpages": "26",
        "venue": "TSE2024",
        "keywords": [
            "program testing",
            "survey"
        ]
    },
    "Code Review Automation: Strengths and Weaknesses of the State of the Art": {
        "type": "article",
        "key": "10.1109/TSE.2023.3348172",
        "author": "Tufano, Rosalia and Dabi\\'{c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele",
        "title": "Code Review Automation: Strengths and Weaknesses of the State of the Art",
        "year": "2024",
        "issue_date": "Feb. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "2",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3348172",
        "doi": "10.1109/TSE.2023.3348172",
        "abstract": "The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques &lt;italic&gt;imitating&lt;/italic&gt; developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, &lt;italic&gt;e.g.,&lt;/italic&gt; the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques\u2019 capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10\\% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$sim$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mo&gt;\u223c&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"bavota-ieq1-3348172.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "jan",
        "pages": "338\u2013353",
        "numpages": "16",
        "venue": "TSE2024",
        "keywords": [
            "code review",
            "empirical study"
        ]
    },
    "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation": {
        "type": "article",
        "key": "10.1109/TSE.2023.3334955",
        "author": "Sch\\\"{a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank",
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "year": "2023",
        "issue_date": "Jan. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "1",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2023.3334955",
        "doi": "10.1109/TSE.2023.3334955",
        "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in &lt;sc&gt;TestPilot&lt;/sc&gt;, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate &lt;sc&gt;TestPilot&lt;/sc&gt; using OpenAI's &lt;italic&gt;gpt3.5-turbo&lt;/italic&gt; LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\\% and branch coverage of 52.8\\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\\% statement coverage and 25.6\\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\\% of &lt;sc&gt;TestPilot&lt;/sc&gt;'s generated tests have &lt;inline-formula&gt;&lt;tex-math notation=\"LaTeX\"&gt;$leq$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math display=\"inline\"&gt;&lt;mml:mo&gt;\u2264&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; 50\\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run &lt;sc&gt;TestPilot&lt;/sc&gt; with two additional LLMs, OpenAI's older &lt;italic&gt;code-cushman-002&lt;/italic&gt; LLM and &lt;italic&gt;StarCoder&lt;/italic&gt;, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\\% median statement coverage), and somewhat worse results with the latter (54.0\\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "nov",
        "pages": "85\u2013105",
        "numpages": "21",
        "venue": "TSE2024",
        "keywords": [
            "program testing",
            "unit testing",
            "empirical study"
        ]
    },
    "Learning to Generate Structured Code Summaries From Hybrid Code Context": {
        "type": "article",
        "key": "10.1109/TSE.2024.3439562",
        "author": "Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie",
        "title": "Learning to Generate Structured Code Summaries From Hybrid Code Context",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3439562",
        "doi": "10.1109/TSE.2024.3439562",
        "abstract": "Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the \u201cone-to-one\u201d mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70\\% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2512\u20132528",
        "numpages": "17",
        "venue": "TSE2024",
        "keywords": [
            "static analysis",
            "code summarization",
            "benchmark"
        ]
    },
    "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration": {
        "type": "article",
        "key": "10.1109/TSE.2024.3445338",
        "author": "Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert",
        "title": "Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3445338",
        "doi": "10.1109/TSE.2024.3445338",
        "abstract": "Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of &lt;italic&gt;follow-up attention&lt;/italic&gt; which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47\\% accuracy. This outperforms the baseline prediction accuracy of 42.3\\%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "aug",
        "pages": "2568\u20132582",
        "numpages": "15",
        "venue": "TSE2024",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model",
            "benchmark"
        ]
    },
    "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction": {
        "type": "article",
        "key": "10.1109/TSE.2024.3450837",
        "author": "Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction",
        "year": "2024",
        "issue_date": "Oct. 2024",
        "publisher": "IEEE Press",
        "volume": "50",
        "number": "10",
        "issn": "0098-5589",
        "url": "https://doi.org/10.1109/TSE.2024.3450837",
        "doi": "10.1109/TSE.2024.3450837",
        "abstract": "Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique &lt;sc&gt;Libro&lt;/sc&gt; could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70\\% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90\\% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using &lt;sc&gt;Libro&lt;/sc&gt; improves as LLM size increases, providing information as to which LLMs can be used with the &lt;sc&gt;Libro&lt;/sc&gt; pipeline.",
        "journal": "IEEE Trans. Softw. Eng.",
        "month": "sep",
        "pages": "2677\u20132694",
        "numpages": "18",
        "venue": "TSE2024",
        "keywords": [
            "program testing",
            "bug reproduction",
            "empirical study"
        ]
    },
    "DeGPT: Optimizing Decompiler Output with LLM": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Peiwei Hu and Chinese Academy of Sciences and Beijing and China) and Ruigang Liang and Chinese Academy of Sciences and Beijing and China) and Kai Chen and Chinese Academy of Sciences and China)",
        "booktitle": "NDSS2024",
        "title": "DeGPT: Optimizing Decompiler Output with LLM",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reverse engineering is essential in malware analysis, vulnerability discovery, etc. Decompilers assist the reverse engineers by lifting the assembly to the high-level programming language, which highly boosts binary comprehension. However, decompilers suffer from problems such as meaningless variable names, redundant variables, and lacking comments describing the purpose of the code. Previous studies have shown promising performance in refining the decompiler output by training the models with huge datasets containing various decompiler outputs. However, even datasets that take much time to construct cover limited binaries in the real world. The performance degrades severely facing the binary migration.In this paper, we present DeGPT, an end-to-end framework aiming to optimize the decompiler output to improve its readability and simplicity and further assist the reverse engineers in understanding the binaries better. The Large Language Model (LLM) can mitigate performance degradation with its extraordinary ability endowed by large model size and training set containing rich multi-modal data. However, its potential is difficult to unlock through one-shot use. Thus, we propose the three-role mechanism, which includes referee (R_ref), advisor (R_adv), and operator (R_ope), to adapt the LLM to our optimization tasks. Specifically, R_ref provides the optimization scheme for the target decompiler output, while R_adv gives the rectification measures based on the scheme, and R_ope inspects whether the optimization changes the original function semantics and concludes the final verdict about whether to accept the optimizations. We evaluate DeGPT on the datasets containing decompiler outputs of various software, such as the practical command line tools, malware, a library for audio processing, and implementations of algorithms. The experimental results show that even on the output of the current top-level decompiler (Ghidra), DeGPT can achieve 24.4% reduction in the cognitive burden of understanding the decompiler outputs and provide comments of which 62.9% can provide practical semantics for the reverse engineers to help the understanding of binaries. Our user surveys also show that the optimizations can significantly simplify the code and add helpful semantic information (variable names and comments), facilitating a quick and accurate understanding of the binary.",
        "keywords": [
            "code generation",
            "program decompilation"
        ],
        "url": "https://www.ndss-symposium.org/ndss-paper/degpt-optimizing-decompiler-output-with-llm",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2024"
    },
    "Large Language Model guided Protocol Fuzzing": {
        "type": "INPROCEEDINGS",
        "key": "",
        "author": "Ruijie Meng and Singapore) and Martin Mirchev and Marcel B\u00f6hme and Germany and Monash University and Australia) and Abhik Roychoudhury",
        "booktitle": "NDSS2024",
        "title": "Large Language Model guided Protocol Fuzzing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "How to find security flaws in a protocol implementation without a machine-readable specification of the protocol? Facing the internet, protocol implementations are particularly security-critical software systems where inputs must adhere to a specific structure and order that is often informally specified in hundreds of pages in natural language (RFC). Without some machine-readable version of that protocol, it is difficult to automatically generate valid test inputs for its implementation that follow the required structure and order. It is possible to partially alleviate this challenge using mutational fuzzing on a set of recorded message sequences as seed inputs. However, the set of available seeds is often quite limited and will hardly cover the great diversity of protocol states and input structures.In this paper, we explore the opportunities of systematic interaction with a pre-trained large language models (LLM) which has ingested millions of pages of human-readable protocol specifications, to draw out machine-readable information about the protocol that can be used during protocol fuzzing.  We use the knowledge of the LLMs about protocol message types for well-known protocols. We also checked the LLM's capability in detecting ``states\" for stateful protocol implementations by generating sequences of messages and predicting response codes. Based on these observations, we have developed an LLM-guided protocol implementation fuzzing engine. Our protocol fuzzer ChatAFL constructs grammars for each message type in a protocol, and then mutates messages or predicts the next messages in a message sequence via interactions with LLMs. Experiments on a wide range of real-world protocols from ProFuzzbench show significant efficacy in state and code coverage. Our LLM-guided stateful fuzzer was compared with state-of-the-art fuzzers AFLNet and NSFuzz. ChatAFL covers 47.6% and 42.7% more state transitions, 29.6% and 25.8% more states, and 5.8% and 6.7% more code, respectively. Apart from enhanced coverage, ChatAFL discovered nine distinct and previously unknown vulnerabilities in widely-used and extensively-tested protocol implementations while AFLNet and NSFuzz only discover three and four of them, respectively.",
        "keywords": [
            "program testing",
            "fuzzing",
            "protocol fuzzing"
        ],
        "url": "https://www.ndss-symposium.org/ndss-paper/large-language-model-guided-protocol-fuzzing",
        "doi": "",
        "ISSN": "",
        "month": "",
        "venue": "NDSS2024"
    },
    "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652106",
        "author": "Shan, Shiwen and Huo, Yintong and Su, Yuxin and Li, Yichen and Li, Dan and Zheng, Zibin",
        "title": "Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652106",
        "doi": "10.1145/3650212.3652106",
        "abstract": "Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models (LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91\\%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "13\u201325",
        "numpages": "13",
        "keywords": [
            "software maintenance and deployment",
            "software configuration"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652115",
        "author": "Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun",
        "title": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652115",
        "doi": "10.1145/3650212.3652115",
        "abstract": "In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "124\u2013136",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program testing",
            "bug detection",
            "benchmark"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652124",
        "author": "Wen, Xin-Cheng and Gao, Cuiyun and Gao, Shuzheng and Xiao, Yang and Lyu, Michael R.",
        "title": "SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652124",
        "doi": "10.1145/3650212.3652124",
        "abstract": "Recently, there has been a growing interest in automatic software vulnerability detection.     Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities.     However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects.     First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers.     Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection.         To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named . The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs).Specifically, comprises three main modules:     (1) Comment Tree Construction, which aims at enhancing the model\u2019s ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs.     (2) Structured Natural Language Comment Tree Construction, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree.     (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns.     Experimental results demonstrate that outperforms the best-performing baseline, including the pre-trained model and LLMs, with improvements of 2.96\\%, 13.47\\%, and 3.75\\% in terms of F1 score on the FFMPeg+Qemu, Reveal, and SVulD datasets, respectively. Furthermore, can be applied to different pre-trained models, such as CodeBERT and UniXcoder, yielding the F1 score performance enhancements ranging from 1.37\\% to 10.87\\%.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "235\u2013247",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "LPR: Large Language Models-Aided Program Reduction": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652126",
        "author": "Zhang, Mengxiao and Tian, Yongqiang and Xu, Zhenyang and Dong, Yiwen and Tan, Shin Hwei and Sun, Chengnian",
        "title": "LPR: Large Language Models-Aided Program Reduction",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652126",
        "doi": "10.1145/3650212.3652126",
        "abstract": "Program reduction is a widely used technique to facilitate debugging                compilers by automatically minimizing programs that trigger                compiler bugs. Existing program reduction techniques are either                generic to a wide range of languages (such as Perses and Vulcan)                or specifically optimized for one certain language by exploiting                language-specific knowledge (e.g., C-Reduce). However, synergistically                combining both generality across languages and optimality                to a specific language in program reduction is yet to be explored.                This paper proposes LPR, the first LLMs-aided technique leveraging                LLMs to perform language-specific program reduction for                multiple languages. The key insight is to utilize both the language                generality of program reducers such as Perses and the languagespecific                semantics learned by LLMs. Concretely, language-generic                program reducers can efficiently reduce programs into a small size                that is suitable for LLMs to process; LLMs can effectively transform                programs via the learned semantics to create new reduction opportunities                for the language-generic program reducers to further                reduce the programs.                Our thorough evaluation on 50 benchmarks across three programming                languages (i.e., C, Rust and JavaScript) has demonstrated                LPR\u2019s practicality and superiority over Vulcan, the state-of-the-art                language-generic program reducer. For effectiveness, LPR surpasses                Vulcan by producing 24.93\\%, 4.47\\%, and 11.71\\% smaller programs                on benchmarks in C, Rust and JavaScript, separately. Moreover, LPR                and Vulcan have the potential to complement each other. For the C                language for which C-Reduce is optimized, by applying Vulcan to                the output produced by LPR, we can attain program sizes that are                on par with those achieved by C-Reduce. For efficiency perceived                by users, LPR is more efficient when reducing large and complex                programs, taking 10.77\\%, 34.88\\%, 36.96\\% less time than Vulcan to                finish all the benchmarks in C, Rust and JavaScript, separately.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "261\u2013273",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation",
            "program testing",
            "debugging"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Automating Zero-Shot Patch Porting for Hard Forks": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652134",
        "author": "Pan, Shengyi and Wang, You and Liu, Zhongxin and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "Automating Zero-Shot Patch Porting for Hard Forks",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652134",
        "doi": "10.1145/3650212.3652134",
        "abstract": "Forking is a typical way of code reuse, which provides a simple way for developers to create a variant software (denoted as hard fork) by copying and modifying an existing codebase. Despite of the benefits, forking also leads to duplicate efforts in software maintenance. Developers need to port patches across the hard forks to address similar bugs or implement similar features. Due to the divergence between the source project and the hard fork, patch porting is complicated, which requires an adaption regarding different implementations of the same functionality. In this work, we take the first step to automate patch porting for hard forks under a zero-shot setting. We first conduct an empirical study of the patches ported from Vim to Neovim over the last ten years to investigate the necessities of patch porting and the potential flaws in the current practice. We then propose a large language model (LLM) based approach (namely PPatHF) to automatically port patches for hard forks on a function-wise basis. Specifically, PPatHF is composed of a reduction module and a porting module. Given the pre- and post-patch versions of a function from the reference project and the corresponding function from the target project, the reduction module first slims the input functions by removing code snippets less relevant to the patch. Then, the porting module leverages a LLM to apply the patch to the function from the target project. To better elicit the power of the LLM on patch porting, we design a prompt template to enable efficient in-context learning. We further propose an instruction-tuning based training task to better guide the LLM to port the patch and inject task-specific knowledge. We evaluate PPatHF on 310 Neovim patches ported from Vim. The experimental results show that PPatHF outperforms the baselines significantly. Specifically, PPatHF can correctly port 131 (42.3\\%) patches and automate 57\\% of the manual edits required for the developer to port the patch.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "363\u2013375",
        "numpages": "13",
        "keywords": [
            "software maintenance and deployment",
            "empirical study"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652140",
        "author": "Ouyang, Yicheng and Yang, Jun and Zhang, Lingming",
        "title": "Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652140",
        "doi": "10.1145/3650212.3652140",
        "abstract": "As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "440\u2013452",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3652142",
        "author": "Liu, Chenyan and Cai, Yufan and Lin, Yun and Huang, Yuhuan and Pei, Yunrui and Jiang, Bo and Yang, Ping and Dong, Jin Song and Mei, Hong",
        "title": "CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3652142",
        "doi": "10.1145/3650212.3652142",
        "abstract": "Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project.        In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8\\%-85.3\\%, and the edit content with exact match rate of 41.8\\% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57\\% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "466\u2013478",
        "numpages": "13",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Oracle-Guided Program Selection from Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680308",
        "author": "Fan, Zhiyu and Ruan, Haifeng and Mechtaev, Sergey and Roychoudhury, Abhik",
        "title": "Oracle-Guided Program Selection from Large Language Models",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680308",
        "doi": "10.1145/3650212.3680308",
        "abstract": "While large language models (LLMs) have shown significant advancements in code generation, their susceptibility to producing incorrect code poses a significant challenge to the adoption of LLM-generated programs. This issue largely stems from the reliance on natural language descriptions as informal oracles in code generation. Current strategies to mitigate this involve selecting the best program from multiple LLM-generated alternatives, judged by criteria like the consistency of their execution results on an LLM-generated test suite. However, this approach has crucial limitations: (1) LLMs often generate redundant tests or tests that cannot distinguish between correct and incorrect solutions, (2) the used consistency criteria, such as the majority vote, fail to foster developer trust due to the absence of transparent rationale behind the made choices. In this work, we propose a new perspective on increasing the quality of LLM-generated code via program selection using the LLM as a test oracle. Our method is based on our experimentally confirmed observation that LLMs serve more effectively as oracles when tasked with selecting the correct output from multiple choices. Leveraging this insight, we first generate distinguishing inputs that capture semantic discrepancies of programs sampled from an LLM, and record outputs produced by the programs on these inputs. An LLM then selects the most likely to be correct output from these, guided by the natural language problem description. We implemented this idea in a tool LLMCodeChoice and evaluated its accuracy in generating and selecting standalone programs. Our experiments demonstrated its effectiveness in improving pass@1 by 3.6-7\\% on HumanEval and MBPP benchmarks compared to the state-of-art CodeT. Most interestingly, the selected input-output specifications helped us to uncover incompleteness and ambiguities in task descriptions and also identify incorrect ground-truth implementations in the benchmarks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "628\u2013640",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680323",
        "author": "Xia, Chunqiu Steven and Zhang, Lingming",
        "title": "Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680323",
        "doi": "10.1145/3650212.3680323",
        "abstract": "Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM \u2013 ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "819\u2013831",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680328",
        "author": "Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F. and Jin, Shunfu",
        "title": "CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680328",
        "doi": "10.1145/3650212.3680328",
        "abstract": "With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs\u2019 realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM\u2019s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs\u2019 conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2\\%-24.6\\% compared to the baseline, achieving an impressive AVG-5 of 76.6\\% when utilizing GPT-4. These results highlight the potential for enhancing LLMs\u2019 repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors\u2019 workload and improving students\u2019 learning experience, showing promise for code review and other software engineering tasks.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "882\u2013894",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680343",
        "author": "Guo, Lianghong and Wang, Yanlin and Shi, Ensheng and Zhong, Wanjun and Zhang, Hongyu and Chen, Jiachi and Zhang, Ruikai and Ma, Yuchi and Zheng, Zibin",
        "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680343",
        "doi": "10.1145/3650212.3680343",
        "abstract": "Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation task and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation task. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34\\% to 452\\%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1073\u20131085",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680347",
        "author": "Sun, Zhensu and Du, Xiaoning and Yang, Zhou and Li, Li and Lo, David",
        "title": "AI Coders Are among Us: Rethinking Programming Language Grammar towards Efficient Code Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680347",
        "doi": "10.1145/3650212.3680347",
        "abstract": "Artificial Intelligence (AI) models have emerged as another important audience for programming languages alongside humans and machines, as we enter the era of large language models (LLMs). LLMs can now perform well in coding competitions and even write programs like developers to solve various tasks, including mathematical problems. However, the grammar and layout of current programs are designed to cater the needs of human developers -- with many grammar tokens and formatting tokens being used to make the code easier for humans to read. While this is helpful, such a design adds unnecessary computational work for LLMs, as each token they either use or produce consumes computational resources.               To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar.This aims to represent code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python. This allows for not only execution via a modified AST parser, but also seamless transformation between programs written in Python and SimPy, enabling human developers and LLMs to use Python and SimPy, respectively, when they need to collaborate. We also look into methods to help existing LLMs understand and use SimPy effectively. In the experiments, compared with Python, SimPy enables a reduction in token usage by 13.5\\% and 10.4\\% for CodeLlama and GPT-4, respectively, when completing the same set of code-related tasks. Additionally, these models can maintain or even improve their performance when using SimPy instead of Python for these tasks. With these promising results, we call for further contributions to the development of AI-oriented program grammar within our community.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1124\u20131136",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680355",
        "author": "Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang",
        "title": "How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680355",
        "doi": "10.1145/3650212.3680355",
        "abstract": "Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.  To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that:  1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; 2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; 3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.  Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1223\u20131235",
        "numpages": "13",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "ThinkRepair: Self-Directed Automated Program Repair": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680359",
        "author": "Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu",
        "title": "ThinkRepair: Self-Directed Automated Program Repair",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680359",
        "doi": "10.1145/3650212.3680359",
        "abstract": "Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.   To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.   Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\\%\u223c344.4\\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12\u223c65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1274\u20131286",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "SelfPiCo: Self-Guided Partial Code Execution with LLMs": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680368",
        "author": "Xue, Zhipeng and Gao, Zhipeng and Wang, Shaohua and Hu, Xing and Xia, Xin and Li, Shanping",
        "title": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680368",
        "doi": "10.1145/3650212.3680368",
        "abstract": "Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self-Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7\\% and 83.3\\% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9\\% and 33.5\\%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1389\u20131401",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "execution prediction"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Neurosymbolic Repair of Test Flakiness": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680369",
        "author": "Chen, Yang and Jabbarvand, Reyhaneh",
        "title": "Neurosymbolic Repair of Test Flakiness",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680369",
        "doi": "10.1145/3650212.3680369",
        "abstract": "Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to deliver- ing reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order- Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., they leverage program analy- sis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs\u2014 generalizability\u2014and program analysis\u2014soundness\u2014to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57\\% (OD) and 59\\% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8\\% more ID tests than DexFix, 12\\% more OD flaky tests than ODRepair, and 17\\% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12\u201331 \\% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1402\u20131414",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680371",
        "author": "Li, Dong and Yan, Meng and Zhang, Yaosheng and Liu, Zhongxin and Liu, Chao and Zhang, Xiaohong and Chen, Ting and Lo, David",
        "title": "CoSec: On-the-Fly Security Hardening of Code LLMs via Supervised Co-decoding",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680371",
        "doi": "10.1145/3650212.3680371",
        "abstract": "Large Language Models (LLMs) specialized in code have shown exceptional proficiency across various programming-related tasks, particularly code generation. Nonetheless, due to its nature of pretraining on massive uncritically filtered data, prior studies have shown that code LLMs are prone to generate code with potential vulnerabilities. Existing approaches to mitigate this risk involve crafting data without vulnerability and subsequently retraining or fine-tuning the model. As the number of parameters exceeds a billion, the computation and data demands of the above approaches will be enormous. Moreover, an increasing number of code LLMs tend to be distributed as services, where the internal representation is not accessible, and the API is the only way to reach the LLM, making the prior mitigation strategies non-applicable.    To cope with this, we propose CoSec, an on-the-fly Security hardening method of code LLMs based on security model-guided Co-decoding, to reduce the likelihood of code LLMs to generate code containing vulnerabilities. Our key idea is to train a separate but much smaller security model to co-decode with a target code LLM. Since the trained secure model has higher confidence for secure tokens, it guides the generation of the target base model towards more secure code generation. By adjusting the probability distributions of tokens during each step of the decoding process, our approach effectively influences the tendencies of generation without accessing the internal parameters of the target code LLM. We have conducted extensive experiments across various parameters in multiple code LLMs (i.e., CodeGen, StarCoder, and DeepSeek-Coder), and the results show that our approach is effective in security hardening. Specifically, our approach improves the average security ratio of six base models by 5.02\\%-37.14\\%, while maintaining the functional correctness of the target model.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1428\u20131439",
        "numpages": "12",
        "keywords": [
            "code model security",
            "defense"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680379",
        "author": "Cui, Di and Wang, Qiangqiang and Zhao, Yutong and Wang, Jiaqi and Wei, Minjie and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",
        "title": "One-to-One or One-to-Many? Suggesting Extract Class Refactoring Opportunities with Intra-class Dependency Hypergraph Neural Network",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680379",
        "doi": "10.1145/3650212.3680379",
        "abstract": "Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, \u201cone-to-one\u201d relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, \u201cone-to-many\u201d dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers\u2019 actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5\\% in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64\\% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1529\u20131540",
        "numpages": "12",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "AutoCodeRover: Autonomous Program Improvement": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680384",
        "author": "Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik",
        "title": "AutoCodeRover: Autonomous Program Improvement",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680384",
        "doi": "10.1145/3650212.3680384",
        "abstract": "Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM\u2019s understanding of the issue\u2019s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19\\% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1592\u20131604",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680388",
        "author": "Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min",
        "title": "LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680388",
        "doi": "10.1145/3650212.3680388",
        "abstract": "FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency.    In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.\tOur prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin\u2019s superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18\\% and an average of 20\\%\u2212110\\% improvement on business scenario coverage, and up to 93.72\\% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework\u2019s practical applicability and efficiency, marking a significant advancement in FinTech software testing.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1643\u20131655",
        "numpages": "13",
        "keywords": [
            "program testing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680389",
        "author": "Eom, Jueon and Jeong, Seyeon and Kwon, Taekyoung",
        "title": "Fuzzing JavaScript Interpreters with Coverage-Guided Reinforcement Learning for LLM-Based Mutation",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680389",
        "doi": "10.1145/3650212.3680389",
        "abstract": "JavaScript interpreters, crucial for modern web browsers, require an effective fuzzing method to identify security-related bugs. However, the strict grammatical requirements for input present significant challenges. Recent efforts to integrate language models for context- aware mutation in fuzzing are promising but lack the necessary coverage guidance to be fully effective. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with Reinforcement Learning (RL) from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving bug detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results show that CovRL-Fuzz outperforms the state-of-the-art fuzzers in enhancing code coverage and identifying bugs in JavaScript interpreters: CovRL-Fuzz identified 58 real-world security-related bugs in the latest JavaScript interpreters, including 50 previously unknown bugs and 15 CVEs.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1656\u20131668",
        "numpages": "13",
        "keywords": [
            "program testing",
            "fuzzing",
            "compiler testing"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Large Language Models for Equivalent Mutant Detection: How Far Are We?": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680395",
        "author": "Tian, Zhao and Shu, Honglin and Wang, Dong and Cao, Xuejie and Kamei, Yasutaka and Chen, Junjie",
        "title": "Large Language Models for Equivalent Mutant Detection: How Far Are We?",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680395",
        "doi": "10.1145/3650212.3680395",
        "abstract": "Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69\\% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1733\u20131745",
        "numpages": "13",
        "keywords": [
            "program testing",
            "mutation testing",
            "empirical study"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3680397",
        "author": "Yu, Zeliang and Wen, Ming and Guo, Xiaochen and Jin, Hai",
        "title": "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3680397",
        "doi": "10.1145/3650212.3680397",
        "abstract": "As the largest package registry, Node Package Manager (NPM) has become the prime target for various supply chain attacks recently and has been flooded with numerous malicious packages, posing significant security risks to end-users. Learning-based methods have demonstrated promising performance with good adaptability to various types of attacks. However, they suffer from two main limitations. First, they often utilize metadata features or coarse-grained code features extracted at the package level while overlooking complex code semantics. Second, the dataset used to train the model often suffers from a lack of variety both in quantity and diversity, and thus cannot detect significant types of attacks.      To address these problems, we introduce Maltracker, a learningbased NPM malware tracker based on fine-grained features empowered by LLM-enhanced dataset. First, Maltracker constructs precise call graphs to extract suspicious functions that are reachable to a pre-defined set of sensitive APIs, and then utilizes community detection algorithm to identify suspicious code gadgets based on program dependency graph, from which fine-grained features are then extracted. To address the second limitation, we extend the dataset using advanced large language models (LLM) to translate malicious functions from other languages (e.g., C/C++, Python, and Go) into JavaScript. Evaluations shows that Maltracker can achieve an improvement of about 12.6\\% in terms of F1-score at the package level and 31.0\\% at the function level compared with the SOTA learning-based methods. Moreover, the key components of \ud835\udc40\ud835\udc4e\ud835\udc59\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc5f all contribute to the effectiveness of its performance. Finally, Maltracker has also detected 230 new malicious packages in NPM and received 61 thanks letters, among which some contain new malicious behaviors that cannot be detected by existing tools.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1759\u20131771",
        "numpages": "13",
        "keywords": [
            "software maintenance and deployment",
            "supply chain"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685307",
        "author": "Wang, Luqiao and Wang, Qiangqiang and Wang, Jiaqi and Zhao, Yutong and Wei, Minjie and Quan, Zhou and Cui, Di and Li, Qingshan",
        "title": "HECS: A Hypergraph Learning-Based System for Detecting Extract Class Refactoring Opportunities",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685307",
        "doi": "10.1145/3650212.3685307",
        "abstract": "HECS is an advanced tool designed for Extract Class refactoring by leveraging hypergraph learning to model complex dependencies within large classes. Unlike traditional tools that rely on direct one-to-one dependency graphs, HECS uses intra-class dependency hypergraphs to capture one-to-many relationships. This allows HECS to provide more accurate and relevant refactoring suggestions. The tool constructs hypergraphs for each target class, attributes nodes using a pre-trained code model, and trains an enhanced hypergraph neural network. Coupled with a large language model, HECS delivers practical refactoring suggestions. In evaluations on large-scale and real-world datasets, HECS achieved a 38.5\\% increase in precision, 9.7\\% in recall, and 44.4\\% in f1-measure compared to JDeodorant, SSECS, and LLMRefactor. These improvements make HECS a valuable tool for developers, offering practical insights and enhancing existing refactoring techniques.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1851\u20131855",
        "numpages": "5",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "Collaboration to Repository-Level Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3650212.3685562",
        "author": "Wen, Xin-Cheng",
        "title": "Collaboration to Repository-Level Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400706127",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3650212.3685562",
        "doi": "10.1145/3650212.3685562",
        "abstract": "Large Language Model (LLM)-based methods have proven to be effective for many software engineering domains, with a potential for substantial productivity effective for software vulnerability detection.    However, due to the limitation of the length of input contexts of LLM, the existing LLM-based methods mainly focus on detecting function-level and leveraging the in-file context information for vulnerability detection (i.e., intra-procedural vulnerabilities), ignoring the more complex inter-procedural vulnerability detection scenarios in practice.    For instance, in real-world scenarios, developers routinely engage with program analysis to detect vulnerabilities that span multiple cross-file information within repositories.       Since complex processes tend to have redundancy dependencies from spanning multiple files in the repository level and invoking multiple static analysis tools, the ideal goal of vulnerability detection is to extract the vulnerability-related information from the repository and provide potential possible explanations for vulnerability triggers.   However, such a goal is hard to achieve, and thus in this work, we design three works through multi-agent collaboration to approach the goal of repository-level vulnerability detection.",
        "booktitle": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1926\u20131928",
        "numpages": "3",
        "keywords": [
            "code generation",
            "bug detection"
        ],
        "location": "Vienna, Austria",
        "series": "ISSTA 2024",
        "venue": "ISSTA2024"
    },
    "UniLog: Automatic Logging via LLM and In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623326",
        "author": "Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei",
        "title": "UniLog: Automatic Logging via LLM and In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623326",
        "doi": "10.1145/3597503.3623326",
        "abstract": "Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9\\% accuracy in selecting logging positions, (2) 72.3\\% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4\\% of the parameter tuning time needed by fine-tuning the same LLM.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "14",
        "numpages": "12",
        "keywords": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623345",
        "author": "Steenhoek, Benjamin and Gao, Hongyang and Le, Wei",
        "title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623345",
        "doi": "10.1145/3597503.3623345",
        "abstract": "Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "16",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "static analysis",
            "bug detection",
            "code model",
            "source code model"
        ]
    },
    "Large Language Models for Test-Free Fault Localization": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623342",
        "author": "Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent",
        "title": "Large Language Models for Test-Free Fault Localization",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623342",
        "doi": "10.1145/3597503.3623342",
        "abstract": "Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\\%--54.4\\%, and Top-5 results by 14.4\\%-35.6\\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "17",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "program testing",
            "debugging",
            "code model",
            "source code model"
        ]
    },
    "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3608134",
        "author": "Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke",
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3608134",
        "doi": "10.1145/3597503.3608134",
        "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "39",
        "numpages": "13",
        "keywords": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3623343",
        "author": "Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming",
        "title": "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3623343",
        "doi": "10.1145/3597503.3623343",
        "abstract": "Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "70",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "program testing",
            "fuzzing"
        ]
    },
    "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639091",
        "author": "Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang",
        "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639091",
        "doi": "10.1145/3597503.3639091",
        "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models.In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \"code synthesis\" and \"code translation.\" We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "74",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639120",
        "author": "Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Li, Li",
        "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639120",
        "doi": "10.1145/3597503.3639120",
        "abstract": "Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4\\% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5\\% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2\\% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2\\% speedup with only a marginal 1.1\\% reduction in ROUGE-L.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "75",
        "numpages": "12",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "code generation",
            "code completion",
            "empirical study"
        ]
    },
    "Traces of Memorisation in Large Language Models for Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639133",
        "author": "Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie",
        "title": "Traces of Memorisation in Large Language Models for Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639133",
        "doi": "10.1145/3597503.3639133",
        "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47\\% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "78",
        "numpages": "12",
        "keywords": [
            "code model security",
            "attack",
            "benchmark"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Evaluating Large Language Models in Class-Level Code Generation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639219",
        "author": "Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling",
        "title": "Evaluating Large Language Models in Class-Level Code Generation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639219",
        "doi": "10.1145/3597503.3639219",
        "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "81",
        "numpages": "13",
        "keywords": [
            "code generation",
            "benchmark"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639226",
        "author": "Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh",
        "title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639226",
        "doi": "10.1145/3597503.3639226",
        "abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\\% to 47.3\\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "82",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation",
            "empirical study"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639103",
        "author": "Yang, Wenzhang and Song, Linhai and Xue, Yinxing",
        "title": "Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639103",
        "doi": "10.1145/3597503.3639103",
        "abstract": "As a relatively new programming language, Rust is designed to provide both memory safety and runtime performance. To achieve this goal, Rust conducts rigorous static checks against its safety rules during compilation, effectively eliminating memory safety issues that plague C/C++ programs. Although useful, the safety rules pose programming challenges to Rust programmers, since programmers can easily violate safety rules when coding in Rust, leading their code to be rejected by the Rust compiler, a fact underscored by a recent user study. There exists a desire to automate the process of fixing safety-rule violations to enhance Rust's programmability.In this paper, we concentrate on Rust's ownership rules and develop rust-lancet to automatically fix their violations. We devise three strategies for altering code, each intended to modify a Rust program and make it pass Rust's compiler checks. Additionally, we introduce mental semantics to model the behaviors of Rust programs that cannot be compiled due to ownership-rule violations. We design an approach to verify whether modified programs preserve their original behaviors before patches are applied. We apply rust-lancet to 160 safety-rule violations from two sources, successfully fixing 102 violations under the optimal configuration --- more than rustc and six LLM-based techniques. Notably, rust-lancet avoids generating any incorrect patches, a distinction from all other baseline techniques. We also verify the effectiveness of each fixing strategy and behavior preservation validation and affirm the rationale behind these components.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "85",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "PyTy: Repairing Static Type Errors in Python": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639184",
        "author": "Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael",
        "title": "PyTy: Repairing Static Type Errors in Python",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639184",
        "doi": "10.1145/3597503.3639184",
        "abstract": "Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4\\% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "87",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "static analysis",
            "type inference"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Programming Assistant for Exception Handling with CodeBERT": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639188",
        "author": "Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis and Nguyen, Tien",
        "title": "Programming Assistant for Exception Handling with CodeBERT",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639188",
        "doi": "10.1145/3597503.3639188",
        "abstract": "With practical code reuse, the code fragments from developers' forums often migrate to applications. Owing to the incomplete nature of such fragments, they often lack the details on exception handling. The adaptation for exception handling to the codebase is not trivial as developers must learn and memorize what API methods could cause exceptions and what exceptions need to be handled. We propose Neurex, an exception handling recommender that learns from complete code, and accepts a given Java code snippet and recommends 1) if a try-catch block is needed, 2) what statements need to be placed in a try block, and 3) what exception types need to be caught in the catch clause. Inspired by the sequence chunking techniques in natural language processing, we design Neurex via a multi-tasking model with the fine-tuning of the large language model CodeBERT for these three exception handling recommendation tasks. Via the large language model, Neurex can learn the surrounding context, leading to better learning the dependencies among the API elements, and the relations between the statements and the corresponding exception types needed to be handled.Our empirical evaluation shows that Neurex correctly performs all three exception handling recommendation tasks in 71.5\\% of the cases with a F1-score of 70.2\\%, which is a relative improvement of 166\\% over the baseline. It achieves high F1-score from 98.2\\%-99.7\\% in try-catch block necessity checking (a relative improvement of up to 55.9\\% over the baselines). It also correctly decides both the need for try-catch block(s) and the statements to be placed in try blocks with the F1-scores of 74.7\\% and 87.1\\% at the instance and statement levels, an improvement of 129.1\\% and 44.9\\% over the baseline, respectively. Our extrinsic evaluation shows that Neurex relatively improves over the baseline by 56.5\\% in F1-score for detecting exception-related bugs in incomplete Android code snippets.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "94",
        "numpages": "13",
        "keywords": [
            "code generation",
            "program repair",
            "empirical study"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Using an LLM to Help With Code Understanding": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639187",
        "author": "Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad",
        "title": "Using an LLM to Help With Code Understanding",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639187",
        "doi": "10.1145/3597503.3639187",
        "abstract": "Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "97",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "software maintenance and deployment"
        ]
    },
    "Fuzz4All: Universal Fuzzing with Large Language Models": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639121",
        "author": "Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming",
        "title": "Fuzz4All: Universal Fuzzing with Large Language Models",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639121",
        "doi": "10.1145/3597503.3639121",
        "abstract": "Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "126",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "program testing",
            "fuzzing"
        ]
    },
    "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639210",
        "author": "Fu, Jingzhou and Liang, Jie and Wu, Zhiyong and Jiang, Yu",
        "title": "Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639210",
        "doi": "10.1145/3597503.3639210",
        "abstract": "Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly.To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers Sqirrel and Griffin, targeting DBMSs such as Virtuoso, MonetDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to Sqirrel and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46\\%-214.84\\% and 21.40\\%-194.46\\%; compared to Sqirrel and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90\\%-16.20\\% and 9.73\\%-28.41\\%. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "146",
        "numpages": "12",
        "keywords": [
            "program testing",
            "fuzzing",
            "DBMS testing"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639117",
        "author": "Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang",
        "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639117",
        "doi": "10.1145/3597503.3639117",
        "abstract": "Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80\\% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90\\%) for token contracts and acceptable precision (57.14\\%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70\\%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "166",
        "numpages": "13",
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024",
        "keywords": [
            "static analysis",
            "bug detection"
        ]
    },
    "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639202",
        "author": "Sun, Jiamou and Chen, Jieshan and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming",
        "title": "Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639202",
        "doi": "10.1145/3597503.3639202",
        "abstract": "With the widely usage of open-source software, supply-chain-based vulnerability attacks, including SolarWind and Log4Shell, have posed significant risks to software security. Currently, people rely on vulnerability advisory databases or commercial software bill of materials (SBOM) to defend against potential risks. Unfortunately, these datasets do not provide finer-grained file-level vulnerability information, compromising their effectiveness. Previous works have not adequately addressed this issue, and mainstream vulnerability detection methods have their drawbacks that hinder resolving this gap. Driven by the real needs, we propose a framework that can trace the vulnerability-relevant file for each disclosed vulnerability. Our approach uses NVD descriptions with metadata as the inputs, and employs a series of strategies with a LLM model, search engine, heuristic-based text matching method and a deep learning classifier to recommend the most likely vulnerability-relevant file, effectively enhancing the completeness of existing NVD data. Our experiments confirm that the efficiency of the proposed framework, with CodeBERT achieving 0.92 AUC and 0.85 MAP, and our user study proves our approach can help with vulnerability-relevant file detection effectively. To the best of our knowledge, our work is the first one focusing on tracing vulnerability-relevant files, laying the groundwork of building finer-grained vulnerability-aware software bill of materials.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "200",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)": {
        "type": "inproceedings",
        "key": "10.1145/3597503.3639183",
        "author": "Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl",
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "year": "2024",
        "isbn": "9798400702174",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "url": "https://doi.org/10.1145/3597503.3639183",
        "doi": "10.1145/3597503.3639183",
        "abstract": "Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering. Researchers are still learning how to best \"program\" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \"code analysis\" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code \\&amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.",
        "booktitle": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
        "articleno": "220",
        "numpages": "13",
        "keywords": [
            "static analysis",
            "code summarization",
            "prompting strategy",
            "rag"
        ],
        "location": "Lisbon, Portugal",
        "series": "ICSE '24",
        "venue": "ICSE2024"
    },
    "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache": {
        "type": "article",
        "key": "10.1145/3643733",
        "author": "Jiang, Zhihan and Liu, Jinyang and Chen, Zhuangbin and Li, Yichen and Huang, Junjie and Huo, Yintong and He, Pinjia and Gu, Jiazhen and Lyu, Michael R.",
        "title": "LILAC: Log Parsing using LLMs with Adaptive Parsing Cache",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643733",
        "doi": "10.1145/3643733",
        "abstract": "Log parsing transforms log messages into structured formats, serving as the prerequisite step for various log analysis tasks. Although a variety of log parsing approaches have been proposed, their performance on complicated log data remains compromised due to the use of human-crafted rules or learning-based models with limited training data. The recent emergence of powerful large language models (LLMs) demonstrates their vast pre-trained knowledge related to code and logging, making it promising to apply LLMs for log parsing. However, their lack of specialized log parsing capabilities currently hinders their parsing accuracy. Moreover, the inherent inconsistent answers, as well as the substantial overhead, prevent the practical adoption of LLM-based log parsing.   To address these challenges, we propose LILAC, the first practical Log parsIng framework using LLMs with Adaptive parsing Cache. To facilitate accurate and robust log parsing, LILAC leverages the in-context learning (ICL) capability of the LLM by performing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations. Furthermore, LILAC incorporates a novel component, an adaptive parsing cache, to store and refine the templates generated by the LLM. It helps mitigate LLM's inefficiency issue by enabling rapid retrieval of previously processed log templates. In this process, LILAC adaptively updates the templates within the parsing cache to ensure the consistency of parsed results. The extensive evaluation on public large-scale datasets shows that LILAC outperforms state-of-the-art methods by 69.5\\% in terms of the average F1 score of template accuracy. In addition, LILAC reduces the query times to LLMs by several orders of magnitude, achieving a comparable efficiency to the fastest baseline.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "7",
        "numpages": "24",
        "keywords": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "venue": "FSE2024"
    },
    "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises": {
        "type": "article",
        "key": "10.1145/3643745",
        "author": "Zan, Daoguang and Yu, Ailun and Shen, Bo and Chen, Bei and Li, Wei and Gong, Yongshun and Chen, Xiaolin and Yao, Yafen and Luo, Weihua and Guan, Bei and Liu, Yan and Wang, Yongji and Wang, Qianxiang and Cui, Lizhen",
        "title": "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643745",
        "doi": "10.1145/3643745",
        "abstract": "The task of code generation aims to generate code solutions based on given programming problems. Recently, code large language models (code LLMs) have shed new light on this task, owing to their formidable code generation capabilities. While these models are powerful, they seldom focus on further improving the accuracy of library-oriented API invocation. Nonetheless, programmers frequently invoke APIs in routine coding tasks. In this paper, we aim to enhance the proficiency of existing code LLMs regarding API invocation by mimicking analogical learning, which is a critical learning strategy for humans to learn through differences among multiple instances. Motivated by this, we propose a simple yet effective approach, namely DiffCoder, which excels in API invocation by effectively training on the differences (diffs) between analogical code exercises. To assess the API invocation capabilities of code LLMs, we conduct experiments on seven existing benchmarks that focus on mono-library API invocation. Additionally, we construct a new benchmark, namely PanNumEval, to evaluate the performance of multi-library API invocation. Extensive experiments on eight benchmarks demonstrate the impressive performance of DiffCoder. Furthermore, we develop a VSCode plugin for DiffCoder, and the results from twelve invited participants further verify the practicality of DiffCoder.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "19",
        "numpages": "21",
        "keywords": [
            "code generation",
            "code completion"
        ],
        "venue": "FSE2024"
    },
    "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models": {
        "type": "article",
        "key": "10.1145/3643753",
        "author": "Wang, Yan and Li, Xiaoning and Nguyen, Tien N. and Wang, Shaohua and Ni, Chao and Ding, Ling",
        "title": "Natural Is the Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643753",
        "doi": "10.1145/3643753",
        "abstract": "Pre-trained Large Language Models (LLM) have achieved remarkable successes in several domains. However, code-oriented LLMs are often heavy in computational complexity, and quadratically with the length of the input code sequence. Toward simplifying the input program of an LLM, the state-of-the-art approach has the strategies to filter the input code tokens based on the attention scores given by the LLM. The decision to simplify the input program should not rely on the attention patterns of an LLM, as these patterns are influenced by both the model architecture and the pre-training dataset. Since the model and dataset are part of the solution domain, not the problem domain where the input program belongs, the outcome may differ when the model is pre-trained on a different dataset. We propose SlimCode, a model-agnostic code simplification solution for LLMs that depends on the nature of input code tokens. As an empirical study on the LLMs including CodeBERT, CodeT5, and GPT-4 for two main tasks: code search and summarization, we reported that 1) the removal ratio of code has a linear-like relation with the saving ratio on training time, 2) the impact of categorized tokens on code simplification can vary significantly, 3) the impact of categorized tokens on code simplification is task-specific but model-agnostic, and 4) the above findings hold for the paradigm\u2013prompt engineering and interactive in-context learning. The empirical results showed that SlimCode can improve the state-of-the-art technique by 9.46\\% and 5.15\\% in terms of MRR and BLEU score on code search and summarization, respectively. More importantly, SlimCode is 133 times faster than the state-of-the-art approach. Additionally, SlimCode can reduce the cost of invoking GPT-4 by up to 24\\% per API query, while still producing comparable results to those with the original code. With this result, we call for a new direction on code-based, model-agnostic code simplification solutions to further empower LLMs.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "27",
        "numpages": "23",
        "keywords": [
            "code model",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Go Static: Contextualized Logging Statement Generation": {
        "type": "article",
        "key": "10.1145/3643754",
        "author": "Li, Yichen and Huo, Yintong and Zhong, Renyi and Jiang, Zhihan and Liu, Jinyang and Huang, Junjie and Gu, Jiazhen and He, Pinjia and Lyu, Michael R.",
        "title": "Go Static: Contextualized Logging Statement Generation",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643754",
        "doi": "10.1145/3643754",
        "abstract": "Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables.                                To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts.First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement. The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement.                                The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7\\% in logging position accuracy, 32.1\\% in level accuracy, 19.6\\% in variable precision, and 138.4\\% in text BLEU-4 score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "28",
        "numpages": "22",
        "keywords": [
            "software maintenance and deployment",
            "system log analysis"
        ],
        "venue": "FSE2024"
    },
    "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example": {
        "type": "article",
        "key": "10.1145/3643755",
        "author": "Dilhara, Malinda and Bellur, Abhiram and Bryksin, Timofey and Dig, Danny",
        "title": "Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643755",
        "doi": "10.1145/3643755",
        "abstract": "Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as \u201ccode change patterns\u201d (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems.                In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6\\%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83\\% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "29",
        "numpages": "23",
        "keywords": [
            "code generation",
            "program transformation"
        ],
        "venue": "FSE2024"
    },
    "CodePlan: Repository-Level Coding using LLMs and Planning": {
        "type": "article",
        "key": "10.1145/3643757",
        "author": "Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and C., Vageesh D. and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B. and Shet, Shashank",
        "title": "CodePlan: Repository-Level Coding using LLMs and Planning",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643757",
        "doi": "10.1145/3643757",
        "abstract": "Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code.     We formulate these activities as repository-level coding tasks.         Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems.     Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt.     We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it.     CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions.     CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs.         We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2\u201397 files).     Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines.     CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.     We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "31",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design",
            "planning"
        ],
        "venue": "FSE2024"
    },
    "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model": {
        "type": "article",
        "key": "10.1145/3643760",
        "author": "Li, Jiawei and Farag\\'{o}, David and Petrov, Christian and Ahmed, Iftekhar",
        "title": "Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643760",
        "doi": "10.1145/3643760",
        "abstract": "Commit messages play a vital role in software development and maintenance. While previous research has introduced various Commit Message Generation (CMG) approaches, they often suffer from a lack of consideration for the broader software context associated with code changes. This limitation resulted in generated commit messages that contained insufficient information and were poorly readable. To address these shortcomings, we approached CMG as a knowledge-intensive reasoning task. We employed ReAct prompting with a cutting-edge Large Language Model (LLM) to generate high-quality commit messages. Our tool retrieves a wide range of software context information, enabling the LLM to create commit messages that are factually grounded and comprehensive. Additionally, we gathered commit message quality expectations from software practitioners, incorporating them into our approach to further enhance message quality. Human evaluation demonstrates the overall effectiveness of our CMG approach, which we named Omniscient Message Generator (OMG). It achieved an average improvement of 30.2\\% over human-written messages and a 71.6\\% improvement over state-of-the-art CMG methods.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "34",
        "numpages": "22",
        "keywords": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "venue": "FSE2024"
    },
    "CORE: Resolving Code Quality Issues using LLMs": {
        "type": "article",
        "key": "10.1145/3643762",
        "author": "Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram",
        "title": "CORE: Resolving Code Quality Issues using LLMs",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643762",
        "doi": "10.1145/3643762",
        "abstract": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\\% Java files (across 10 quality checks) comparable to 78.3\\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "36",
        "numpages": "23",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "venue": "FSE2024"
    },
    "Towards AI-Assisted Synthesis of Verified Dafny Methods": {
        "type": "article",
        "key": "10.1145/3643763",
        "author": "Misu, Md Rakib Hossain and Lopes, Cristina V. and Ma, Iris and Noble, James",
        "title": "Towards AI-Assisted Synthesis of Verified Dafny Methods",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643763",
        "doi": "10.1145/3643763",
        "abstract": "Large language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises, generating erroneous code. A promising avenue to keep models honest is to incorporate formal verification: generating programs\u2019 specifications as well as code so that the code can be proved correct with respect to the specifications. Unfortunately, existing large language models show a severe lack of proficiency in verified programming.        In this paper, we demonstrate how to improve two pretrained models\u2019 proficiency in the Dafny verification-aware language. Using 178 problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to synthesize Dafny methods. We use three different types of prompts: a direct Contextless prompt; a Signature prompt that includes a method signature and test cases, and a Chain of Thought (CoT) prompt that decomposes the problem into steps and includes retrieval augmentation generated example problems and solutions. Our results show that GPT-4 performs better than PaLM-2 on these tasks and that both models perform best with the retrieval augmentation generated CoT prompt. GPT-4 was able to generate verified, human-evaluated, Dafny methods for 58\\% of the problems, however, GPT-4 managed only 19\\% of the problems with the Contextless prompt, and even fewer (10\\%) for the Signature prompt. We are thus able to contribute 153 verified Dafny solutions to MBPP problems, 50 that we wrote manually, and 103 synthesized by GPT-4.        Our results demonstrate that the benefits of formal program verification are now within reach of code generating large language models. Likewise, program verification systems can benefit from large language models, whether to synthesize code wholesale, to generate specifications, or to act as a \"programmer\u2019s verification apprentice\", to construct annotations such as loop invariants which are hard for programmers to write or verification tools to find. Finally, we expect that the approach we have pioneered here \u2014 generating candidate solutions that are subsequently formally checked for correctness \u2014 should transfer to other domains (e.g., legal arguments, transport signaling, structural engineering) where solutions must be correct, where that correctness must be demonstrated, explained and understood by designers and end-users.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "37",
        "numpages": "24",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ],
        "venue": "FSE2024"
    },
    "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings": {
        "type": "article",
        "key": "10.1145/3643767",
        "author": "Jiang, Weipeng and Zhai, Juan and Ma, Shiqing and Zhang, Xiaoyu and Shen, Chao",
        "title": "COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643767",
        "doi": "10.1145/3643767",
        "abstract": "Large language models have gained significant popularity and are often provided as a service (i.e., LLMaaS).  Companies like OpenAI and Google provide online APIs of LLMs to allow downstream users to create innovative applications.  Despite its popularity, LLM safety and quality assurance is a well-recognized concern in the real world, requiring extra efforts for testing these LLMs.  Unfortunately, while end-to-end services like ChatGPT have garnered rising attention in terms of testing, the LLMaaS embeddings have comparatively received less scrutiny.  We state the importance of testing and uncovering problematic individual embeddings without considering downstream applications.  The abstraction and non-interpretability of embedded vectors, combined with the black-box inaccessibility of LLMaaS, make testing a challenging puzzle.  This paper proposes COSTELLO, a black-box approach to reveal potential defects in abstract embedding vectors from LLMaaS by contrastive testing.  Our intuition is that high-quality LLMs can adequately capture the semantic relationships of the input texts and properly represent their relationships in the high-dimensional space.  For the given interface of LLMaaS and seed inputs, COSTELLO can automatically generate test suites and output words with potential problematic embeddings.  The idea is to synthesize contrastive samples with guidance, including positive and negative samples, by mutating seed inputs.  Our synthesis guide will leverage task-specific properties to control the mutation procedure and generate samples with known partial relationships in the high-dimensional space.  Thus, we can compare the expected relationship (oracle) and embedding distance (output of LLMs) to locate potential buggy cases.  We evaluate COSTELLO on 42 open-source (encoder-based) language models and two real-world commercial LLMaaS.  Experimental results show that COSTELLO can effectively detect semantic violations, where more than 62\\% of violations on average result in erroneous behaviors (e.g., unfairness) of downstream applications.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "41",
        "numpages": "23",
        "keywords": [
            "code model",
            "source code model"
        ],
        "venue": "FSE2024"
    },
    "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM": {
        "type": "article",
        "key": "10.1145/3643769",
        "author": "Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi",
        "title": "Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM",
        "year": "2024",
        "issue_date": "July 2024",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "1",
        "number": "FSE",
        "url": "https://doi.org/10.1145/3643769",
        "doi": "10.1145/3643769",
        "abstract": "Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt\u2019s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.",
        "journal": "Proc. ACM Softw. Eng.",
        "month": "jul",
        "articleno": "43",
        "numpages": "21",
        "keywords": [
            "program testing",
            "fuzzing"
        ],
        "venue": "FSE2024"
    },
    "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search": {
        "type": "INPROCEEDINGS",
        "key": "li-etal-2024-rewriting",
        "author": "Li, Haochen and Zhou, Xin and Shen, Zhiqi",
        "booktitle": "ACL2024",
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at https://github.com/Alex-HaochenLi/ReCo.",
        "keywords": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.75",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-enhancing",
        "author": "Huang, Baizhou and Lu, Shuai and Wan, Xiaojun and Duan, Nan",
        "booktitle": "ACL2024",
        "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs\u2019 reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph.MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.78",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "UniCoder: Scaling Code Large Language Model via Universal Code": {
        "type": "INPROCEEDINGS",
        "key": "sun-etal-2024-unicoder",
        "author": "Sun, Tao and Chai, Linzheng and Yang, Jian and Yin, Yuwei and Guo, Hongcheng and Liu, Jiaheng and Wang, Bing and Yang, Liqun and Li, Zhoujun",
        "booktitle": "ACL2024",
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks.When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "IR model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.100",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Virtual Compiler Is All You Need For Assembly Code Search": {
        "type": "INPROCEEDINGS",
        "key": "gao-etal-2024-virtual",
        "author": "Gao, Zeyu and Wang, Hao and Wang, Yuanda and Zhang, Chao",
        "booktitle": "ACL2024",
        "title": "Virtual Compiler Is All You Need For Assembly Code Search",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs.Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code to assembly code. This approach allows for \u201cvirtual\u201d compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.",
        "keywords": [
            "code generation",
            "program translation",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.167",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning": {
        "type": "INPROCEEDINGS",
        "key": "dai-etal-2024-mpcoder",
        "author": "Dai, Zhenlong and Yao, Chang and Han, WenKang and Yuanying, Yuanying and Gao, Zhipeng and Chen, Jingyuan",
        "booktitle": "ACL2024",
        "title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.207",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback": {
        "type": "INPROCEEDINGS",
        "key": "dou-etal-2024-stepcoder",
        "author": "Dou, Shihan and Liu, Yan and Jia, Haoxiang and Zhou, Enyu and Xiong, Limao and Shan, Junjie and Huang, Caishuang and Wang, Xiao and Fan, Xiaoran and Xi, Zhiheng and Zhou, Yuhao and Ji, Tao and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing",
        "booktitle": "ACL2024",
        "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.251",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-dolphcoder",
        "author": "Wang, Yejie and He, Keqing and Dong, Guanting and Wang, Pei and Zeng, Weihao and Diao, Muxi and Xu, Weiran and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang",
        "booktitle": "ACL2024",
        "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Various instruction finetuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model DolphCoder with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with more distinct reasoning paths increases the code capability of LLMs. (2) Improving one\u2019s ability to evaluate the correctness of code also enhances their ability to create it.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.259",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Who Wrote this Code? Watermarking for Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "lee-etal-2024-wrote",
        "author": "Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee",
        "booktitle": "ACL2024",
        "title": "Who Wrote this Code? Watermarking for Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task\u2019s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
        "keywords": [
            "code generation",
            "program synthesis",
            "code model security",
            "defense"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.268",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving": {
        "type": "INPROCEEDINGS",
        "key": "islam-etal-2024-mapcoder",
        "author": "Islam, Md. Ashraful and Ali, Mohammed Eunus and Parvez, Md Rizwan",
        "booktitle": "ACL2024",
        "title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks\u2014MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) results\u2014(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.",
        "keywords": [
            "code generation",
            "program synthesis",
            "agent design"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.269",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning": {
        "type": "INPROCEEDINGS",
        "key": "yu-etal-2024-wavecoder",
        "author": "Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng",
        "booktitle": "ACL2024",
        "title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",
        "keywords": [
            "code model",
            "source code model"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.280",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation": {
        "type": "INPROCEEDINGS",
        "key": "yan-etal-2024-codescope",
        "author": "Yan, Weixiang and Liu, Haitian and Wang, Yunkun and Li, Yunzhe and Chen, Qian and Wang, Wen and Lin, Tingyu and Zhao, Weishan and Zhu, Li and Sundaram, Hari and Deng, Shuiguang",
        "booktitle": "ACL2024",
        "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.301",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Experiential Co-Learning of Software-Developing Agents": {
        "type": "INPROCEEDINGS",
        "key": "qian-etal-2024-experiential",
        "author": "Qian, Chen and Dang, Yufan and Li, Jiahao and Liu, Wei and Xie, Zihao and Wang, YiFei and Chen, Weize and Yang, Cheng and Cong, Xin and Che, Xiaoyin and Liu, Zhiyuan and Sun, Maosong",
        "booktitle": "ACL2024",
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "keywords": [
            "general coding task",
            "agent design",
            "planning"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.305",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval": {
        "type": "INPROCEEDINGS",
        "key": "khan-etal-2024-xcodeeval",
        "author": "Khan, Mohammad Abdullah Matin and Bari, M. Saiful and Long, Do and Wang, Weishi and Parvez, Md Rizwan and Joty, Shafiq",
        "booktitle": "ACL2024",
        "title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI\u2019s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.",
        "keywords": [
            "general coding task",
            "benchmark"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.367",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Lightweight reranking for language model generations": {
        "type": "INPROCEEDINGS",
        "key": "jain-etal-2024-lightweight",
        "author": "Jain, Siddhartha and Ma, Xiaofei and Deoras, Anoop and Xiang, Bing",
        "booktitle": "ACL2024",
        "title": "Lightweight reranking for language model generations",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.376",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-integrate",
        "author": "Wang, Xinglin and Li, Yiwei and Feng, Shaoxiong and Yuan, Peiwen and Pan, Boyuan and Wang, Heda and Hu, Yao and Li, Kan",
        "booktitle": "ACL2024",
        "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples.",
        "keywords": [
            "general coding task",
            "empirical study"
        ],
        "url": "https://doi.org/10.18653/v1/2024.acl-long.634",
        "doi": "",
        "ISSN": "",
        "month": "August",
        "venue": "ACL2024"
    },
    "Language-to-Code Translation with a Single Labeled Example": {
        "type": "INPROCEEDINGS",
        "key": "bostrom-etal-2024-language",
        "author": "Bostrom, Kaj and Jhamtani, Harsh and Fang, Hao and Thomson, Sam and Shin, Richard and Xia, Patrick and Van Durme, Benjamin and Eisner, Jason and Andreas, Jacob",
        "booktitle": "EMNLP-main2024",
        "title": "Language-to-Code Translation with a Single Labeled Example",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a \u201cnearly unsupervised\u201d setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85% of the performance of a fully supervised system.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.462",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?": {
        "type": "INPROCEEDINGS",
        "key": "cao-etal-2024-realvul",
        "author": "Cao, Di and Liao, Yong and Shang, Xiuwei",
        "booktitle": "EMNLP-main2024",
        "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in data sampling and processing persist, hindering the model\u2019s ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By improving code sampling methods and employing normalization techniques, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul\u2019s performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",
        "keywords": [
            "static analysis",
            "bug detection"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.472",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs": {
        "type": "INPROCEEDINGS",
        "key": "puerto-etal-2024-code",
        "author": "Puerto, Haritz and Tutek, Martin and Aditya, Somak and Zhu, Xiaodan and Gurevych, Iryna",
        "booktitle": "EMNLP-main2024",
        "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs\u2019 performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we investigate the effect of the input representation on the reasoning abilities of LLMs. We hypothesize that representing natural language tasks as code can enhance specific reasoning abilities such as entity tracking or logical reasoning. To study this, we propose code prompting, a methodology we operationalize as a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how the code representation triggers reasoning abilities and which capabilities are elicited in the underlying models. Our analysis on GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, the code representation improves sample efficiency of in-context learning and facilitates state tracking of entities.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.629",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "CodeAgent: Autonomous Communicative Agents for Code Review": {
        "type": "INPROCEEDINGS",
        "key": "tang-etal-2024-codeagent",
        "author": "Tang, Xunzhu and Kim, Kisub and Song, Yewei and Lothritz, Cedric and Li, Bei and Ezzini, Saad and Tian, Haoye and Klein, Jacques and Bissyand\u00e9, Tegawend\u00e9 F.",
        "booktitle": "EMNLP-main2024",
        "title": "CodeAgent: Autonomous Communicative Agents for Code Review",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces CodeAgent, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents\u2019 contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revisions. The results demonstrate CodeAgent\u2019s effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (https://github.com/Daniel4SE/codeagent).",
        "keywords": [
            "software maintenance and deployment",
            "code review",
            "agent design"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.632",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models": {
        "type": "INPROCEEDINGS",
        "key": "ranaldi-etal-2024-empowering-multi",
        "author": "Ranaldi, Leonardo and Pucci, Giulia and Haddow, Barry and Birch, Alexandra",
        "booktitle": "EMNLP-main2024",
        "title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), which elicit LLMs to generate reasoning paths, thus promoting accuracy and attracting increasing attention. However, despite the success of these methods, the ability to deliver multi-step reasoning remains limited to a single language, making it challenging to generalize to other languages and hindering global development.In this work, we propose Cross-lingual Program-Aided Language Models (CrossPAL), a method for aligning reasoning programs across languages. In particular, our method delivers programs as intermediate reasoning steps in different languages through a double-step cross-lingual prompting mechanism inspired by the Program-Aided approach. In addition, we introduce Self-consistent CrossPAL (SCrossPAL) to ensemble different reasoning paths across languages. Our experimental evaluations show that our method significantly outperforms existing prompting methods, reducing the number of interactions and achieving state-of-the-art performance.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.678",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "huang-etal-2024-da",
        "author": "Huang, Yiming and Luo, Jianwen and Yu, Yan and Zhang, Yitong and Lei, Fangyu and Wei, Yifan and He, Shizhu and Huang, Lifu and Liu, Xiao and Zhao, Jun and Liu, Kang",
        "booktitle": "EMNLP-main2024",
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.748",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Leveraging Context-Aware Prompting for Commit Message Generation": {
        "type": "INPROCEEDINGS",
        "key": "jiang-etal-2024-leveraging-context",
        "author": "Jiang, Zhihua and Chen, Jianwei and Rao, Dongning and Ye, Guanghui",
        "booktitle": "EMNLP-main2024",
        "title": "Leveraging Context-Aware Prompting for Commit Message Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Writing comprehensive commit messages is tedious yet important, because these messages describe changes of code, such as fixing bugs or adding new features. However, most existing methods focus on either only the changed lines or nearest context lines, without considering the effectiveness of selecting useful contexts. On the other hand, it is possible that introducing excessive contexts can lead to noise. To this end, we propose a code model COMMIT (Context-aware prOMpting based comMIt-message generaTion) in conjunction with a code dataset CODEC (COntext and metaData Enhanced Code dataset). Leveraging program slicing, CODEC consolidates code changes along with related contexts via property graph analysis. Further, utilizing CodeT5+ as the backbone model, we train COMMIT via context-aware prompt on CODEC. Experiments show that COMMIT can surpass all compared models including pre-trained language models for code (code-PLMs) such as CommitBART and large language models for code (code-LLMs) such as Code-LlaMa. Besides, we investigate several research questions (RQs), further verifying the effectiveness of our approach. We release the data and code at: https://github.com/Jnunlplab/COMMIT.git.",
        "keywords": [
            "software maintenance and deployment",
            "commit message generation"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.749",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data": {
        "type": "INPROCEEDINGS",
        "key": "wang-etal-2024-code",
        "author": "Wang, Yejie and He, Keqing and Fu, Dayuan and GongQue, Zhuoma and Xu, Heyang and Chen, Yanxu and Wang, Zhexu and Fu, Yujia and Dong, Guanting and Diao, Muxi and Wang, Jingang and Zhang, Mengdi and Cai, Xunliang and Xu, Weiran",
        "booktitle": "EMNLP-main2024",
        "title": "How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show Xcoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs.",
        "keywords": [
            "code model",
            "source code model"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.777",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?": {
        "type": "INPROCEEDINGS",
        "key": "waghjale-etal-2024-ecco",
        "author": "Waghjale, Siddhant and Veerendranath, Vishruth and Wang, Zhiruo and Fried, Daniel",
        "booktitle": "EMNLP-main2024",
        "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.859",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Re-Reading Improves Reasoning in Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "xu-etal-2024-reading",
        "author": "Xu, Xiaohan and Tao, Chongyang and Shen, Tao and Xu, Can and Xu, Hongbo and Long, Guodong and Lou, Jian-Guang and Ma, Shuai",
        "booktitle": "EMNLP-main2024",
        "title": "Re-Reading Improves Reasoning in Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a \u201cbidirectional\u201d encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable \u201cbidirectional\u201d attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2\u2019s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.",
        "keywords": [
            "prompting strategy"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.871",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?": {
        "type": "INPROCEEDINGS",
        "key": "uchiyama-etal-2024-programming",
        "author": "Uchiyama, Fumiya and Kojima, Takeshi and Gambardella, Andrew and Cao, Qi and Iwasawa, Yusuke and Matsuo, Yutaka",
        "booktitle": "EMNLP-main2024",
        "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks.Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.",
        "keywords": [
            "code model",
            "source code model",
            "empirical study"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1008",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "DocCGen: Document-based Controlled Code Generation": {
        "type": "INPROCEEDINGS",
        "key": "pimparkhede-etal-2024-doccgen",
        "author": "Pimparkhede, Sameer and Kammakomati, Mehant and Tamilselvam, Srikanth G. and Kumar, Prince and Kumar, Ashok Pon and Bhattacharyya, Pushpak",
        "booktitle": "EMNLP-main2024",
        "title": "DocCGen: Document-based Controlled Code Generation",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Recent developments show that Large Language Models (LLMs) produce state-of-the-art performance on natural language (NL) to code generation for resource-rich general-purpose languages like C++, Java, and Python. However, their practical usage for structured domain-specific languages (DSLs) such as YAML, JSON is limited due to domain-specific schema, grammar, and customizations generally unseen by LLMs during pre-training. Efforts have been made to mitigate this challenge via in-context learning through relevant examples or by fine-tuning. However, it suffers from problems, such as limited DSL samples and prompt sensitivity but enterprises maintain good documentation of the DSLs. Therefore, we propose DocCGen, a framework that can leverage such rich knowledge by breaking the NL-to-Code generation task for structured code languages into a two-step process. First, it detects the correct libraries using the library documentation that best matches the NL query. Then, it utilizes schema rules extracted from the documentation of these libraries to constrain the decoding. We evaluate our framework for two complex structured languages, Ansible YAML and Bash command, consisting of two settings: Out-of-domain (OOD) and In domain (ID). Our extensive experiments show that DocCGen consistently improves different sized language models across all six evaluation metrics, reducing syntactic and semantic errors in structured code.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1040",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing": {
        "type": "INPROCEEDINGS",
        "key": "he-etal-2024-cocost",
        "author": "He, Xinyi and Zou, Jiaru and Lin, Yun and Zhou, Mengyu and Han, Shi and Yuan, Zejian and Zhang, Dongmei",
        "booktitle": "EMNLP-main2024",
        "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.",
        "keywords": [
            "code generation",
            "program synthesis",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1082",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "CodeJudge: Evaluating Code Generation with Large Language Models": {
        "type": "INPROCEEDINGS",
        "key": "tong-zhang-2024-codejudge",
        "author": "Tong, Weixi and Zhang, Tianyi",
        "booktitle": "EMNLP-main2024",
        "title": "CodeJudge: Evaluating Code Generation with Large Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Large Language Models (LLMs) have shown promising performance in code generation. However, how to reliably evaluate code generated by LLMs remains an unresolved problem. This paper presents CodeJudge, a code evaluation framework that leverages LLMs to evaluate the semantic correctness of generated code without the need for test cases. We investigate different ways to guide the LLM in performing \u201cslow thinking\u201d to arrive at an in-depth and reliable evaluation. We experimented with four LLMs as evaluators on four code generation datasets and five programming languages. The results show that CodeJudge significantly outperformed existing methods in most settings. Furthermore, compared with a SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results even when using a much smaller model, Llama-3-8B-Instruct. Our code and datasets are available on GitHub https://github.com/VichyTong/CodeJudge.",
        "keywords": [
            "code generation",
            "program synthesis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1118",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records": {
        "type": "INPROCEEDINGS",
        "key": "shi-etal-2024-ehragent",
        "author": "Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Zhang, Jieyu and Wu, Hang and Zhu, Yuanda and Ho, Joyce C. and Yang, Carl and Wang, May Dongmei",
        "booktitle": "EMNLP-main2024",
        "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.",
        "keywords": [
            "prompting strategy",
            "reason with code"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1245",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-language",
        "author": "Chae, Hyungjoo and Kim, Yeonghyeon and Kim, Seungone and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Kim, Moohyeon and Kim, Sunghwan and Kwon, Taeyoon and Chung, Jiwan and Yu, Youngjae and Yeo, Jinyoung",
        "booktitle": "EMNLP-main2024",
        "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "Algorithmic reasoning tasks that involve complex logical patterns, such as completing Dyck language, pose challenges for large language models (LLMs), despite their recent success. Prior work has used LLMs to generate programming language and applied external compilers for such tasks. Yet, when on the fly, it is hard to generate an executable code with the correct logic for the solution. Even so, code for one instance cannot be reused for others, although they might require the same logic to solve. We present Think-and-Execute, a novel framework that improves LLMs\u2019 algorithmic reasoning: (1) In Think, we discover task-level logic shared across all instances, and express such logic with pseudocode; (2) In Execute, we tailor the task-level pseudocode to each instance and simulate the execution of it. Think-and-Execute outperforms several strong baselines (including CoT and PoT) in diverse algorithmic reasoning tasks. We manifest the advantage of using task-level pseudocode over generating instance-specific solutions one by one. Also, we show that pseudocode can better improve LMs\u2019 reasoning than natural language (NL) guidance, even though they are trained with NL instructions.",
        "keywords": [
            "static analysis",
            "foundamental analysis"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1253",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code": {
        "type": "INPROCEEDINGS",
        "key": "chae-etal-2024-coffee",
        "author": "Chae, Hyungjoo and Kwon, Taeyoon and Moon, Seungjun and Song, Yongho and Kang, Dongjin and Ong, Kai Tzu-iunn and Kwak, Beong-woo and Bae, Seonghyeon and Hwang, Seung-won and Yeo, Jinyoung",
        "booktitle": "EMNLP-main2024",
        "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
        "year": "2024",
        "volume": "",
        "number": "",
        "pages": "",
        "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans\u2019 code edit traces for coding questions and human-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs\u2019 code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available in https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym.",
        "keywords": [
            "code generation",
            "program repair",
            "benchmark"
        ],
        "url": "https://aclanthology.org/2024.emnlp-main.1254",
        "doi": "",
        "ISSN": "",
        "month": "November",
        "venue": "EMNLP-main2024"
    },
    "Security of Language Models for Code: A Systematic Literature Review": {
        "type": "article",
        "key": "chen2024security",
        "title": "Security of Language Models for Code: A Systematic Literature Review",
        "author": "Chen, Yuchen and Sun, Weisong and Fang, Chunrong and Chen, Zhenpeng and Ge, Yifei and Han, Tingxu and Zhang, Quanjun and Liu, Yang and Chen, Zhenyu and Xu, Baowen",
        "journal": "arXiv preprint arXiv:2410.15631",
        "year": "2024",
        "volume": "",
        "number": "",
        "venue": "arXiv2024",
        "abstract": "Language models for code (CodeLMs) have emerged as powerful tools for code-related tasks, outperforming traditional methods and standard machine learning approaches. However, these models are susceptible to security vulnerabilities, drawing increasing research attention from domains such as software engineering, artificial intelligence, and cybersecurity. Despite the growing body of research focused on the security of CodeLMs, a comprehensive survey in this area remains absent. To address this gap, we systematically review 67 relevant papers, organizing them based on attack and defense strategies. Furthermore, we provide an overview of commonly used language models, datasets, and evaluation metrics, and highlight open-source tools and promising directions for future research in securing CodeLMs.",
        "keywords": [
            "code model security",
            "survey"
        ]
    },
    "LLMs: Understanding Code Syntax and Semantics for Code Analysis": {
        "type": "article",
        "key": "ma2023lms",
        "title": "LLMs: Understanding Code Syntax and Semantics for Code Analysis",
        "author": "Ma, Wei and Liu, Shangqing and Lin, Zhihao and Wang, Wenhan and Hu, Qiang and Liu, Ye and Zhang, Cen and Nie, Liming and Li, Li and Liu, Yang",
        "journal": "arXiv preprint arXiv:2305.12138",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Large language models~(LLMs) demonstrate significant potential to revolutionize software engineering (SE) by exhibiting outstanding performance in SE tasks such as code and document generation. However, the high reliability and risk control requirements in software engineering raise concerns about the lack of interpretability of LLMs. To address this concern, we conducted a study to evaluate the capabilities of LLMs and their limitations for code analysis in SE. We break down the abilities needed for artificial intelligence~(AI) models to address SE tasks related to code analysis into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on the ability of LLMs to comprehend code syntax and semantic structures, which include abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We employed four state-of-the-art foundational models, GPT4, GPT3.5, StarCoder and CodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while LLMs have a talent for understanding code syntax, they struggle with comprehending code semantics, particularly dynamic semantics. We conclude that LLMs possess capabilities similar to an Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static code analysis. Furthermore, our study highlights that LLMs are susceptible to hallucinations when interpreting code semantic structures and fabricating nonexistent facts. These results indicate the need to explore methods to verify the correctness of LLM output to ensure its dependability in SE. More importantly, our study provides an initial answer to why the codes generated by LLM are usually syntax-correct but vulnerable.",
        "keywords": [
            "code model",
            "source code model",
            "empirical study"
        ]
    },
    "Codemind: A framework to challenge large language models for code reasoning": {
        "type": "article",
        "key": "liu2024codemind",
        "title": "Codemind: A framework to challenge large language models for code reasoning",
        "author": "Liu, Changshu and Zhang, Shizhuo Dylan and Ibrahimzada, Ali Reza and Jabbarvand, Reyhaneh",
        "journal": "arXiv preprint arXiv:2402.09664",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly follow control flow constructs and, in general, explain how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.",
        "keywords": [
            "general coding task",
            "empirical study"
        ]
    },
    "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?": {
        "type": "inproceedings",
        "key": "velasco2024syntactic",
        "title": "Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?",
        "author": "Velasco, Alejandro and Palacio, David N and Rodriguez-Cardenas, Daniel and Poshyvanyk, Denys",
        "booktitle": "Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results",
        "pages": "72--76",
        "year": "2024",
        "abstract": "This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs' accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.",
        "venue": "ICSE2024",
        "keywords": [
            "static analysis",
            "fundamental analysis",
            "empirical study"
        ]
    },
    "Grounded Copilot: How Programmers Interact with Code-Generating Models": {
        "type": "article",
        "key": "10.1145/3586030",
        "author": "Barke, Shraddha and James, Michael B. and Polikarpova, Nadia",
        "title": "Grounded Copilot: How Programmers Interact with Code-Generating Models",
        "year": "2023",
        "issue_date": "April 2023",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": "7",
        "number": "OOPSLA1",
        "url": "https://doi.org/10.1145/3586030",
        "doi": "10.1145/3586030",
        "abstract": "Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants\u2014with a range of prior experience using the assistant\u2014as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.",
        "journal": "Proc. ACM Program. Lang.",
        "month": "apr",
        "articleno": "78",
        "numpages": "27",
        "keywords": [
            "code generation",
            "code completion",
            "empirical study"
        ],
        "venue": "OOPLSA2023"
    },
    "SemCoder: Training Code Language Models with Comprehensive Semantics": {
        "type": "article",
        "key": "ding2024semcoder",
        "title": "SemCoder: Training Code Language Models with Comprehensive Semantics",
        "author": "Ding, Yangruibo and Peng, Jinjun and Min, Marcus J and Kaiser, Gail and Yang, Junfeng and Ray, Baishakhi",
        "journal": "arXiv preprint arXiv:2406.01006",
        "year": "2024",
        "abstract": "Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.",
        "venue": "NeurIPS2024",
        "keywords": [
            "general coding task",
            "code model",
            "source code model"
        ]
    },
    "CodeFort: Robust Training for Code Generation Models": {
        "type": "article",
        "key": "zhang2024codefort",
        "title": "CodeFort: Robust Training for Code Generation Models",
        "author": "Zhang, Yuhao and Wang, Shiqi and Qian, Haifeng and Wang, Zijian and Shang, Mingyue and Liu, Linbo and Gouda, Sanjay Krishna and Ray, Baishakhi and Ramanathan, Murali Krishna and Ma, Xiaofei and others",
        "journal": "arXiv preprint arXiv:2405.01567",
        "year": "2024",
        "abstract": "Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%",
        "venue": "EMNLP2024",
        "keywords": [
            "code generation",
            "code model",
            "attack",
            "defense"
        ]
    },
    "Constrained Decoding for Secure Code Generation": {
        "type": "article",
        "key": "fu2024constrained",
        "title": "Constrained Decoding for Secure Code Generation",
        "author": "Fu, Yanjun and Baker, Ethan and Ding, Yu and Chen, Yizheng",
        "journal": "arXiv preprint arXiv:2405.00218",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation. This paper introduces a new benchmark, CodeGuard+, along with two new metrics, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs. Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate secure code. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, our evaluations over eight state-of-the-art Code LLMs show that constrained decoding has strong performance to improve the security of Code LLMs, and our technique outperforms GPT-4.",
        "keywords": [
            "code generation",
            "code model security",
            "defense"
        ]
    },
    "Instruction tuning for secure code generation": {
        "type": "article",
        "key": "he2024instruction",
        "title": "Instruction tuning for secure code generation",
        "author": "He, Jingxuan and Vero, Mark and Krasnopolska, Gabriela and Vechev, Martin",
        "journal": "arXiv preprint arXiv:2402.09497",
        "year": "2024",
        "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",
        "venue": "ICML2024",
        "keywords": [
            "code generation",
            "code model security",
            "defense"
        ]
    },
    "Large language models for code: Security hardening and adversarial testing": {
        "type": "inproceedings",
        "key": "he2023large",
        "title": "Large language models for code: Security hardening and adversarial testing",
        "author": "He, Jingxuan and Vechev, Martin",
        "booktitle": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
        "pages": "1865--1879",
        "year": "2023",
        "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
        "venue": "CCS2023",
        "keywords": [
            "code generation",
            "code model security",
            "defense",
            "attack"
        ]
    },
    "Graphcodebert: Pre-training code representations with data flow": {
        "type": "article",
        "key": "guo2020graphcodebert",
        "title": "Graphcodebert: Pre-training code representations with data flow",
        "author": "Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others",
        "journal": "arXiv preprint arXiv:2009.08366",
        "year": "2020",
        "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
        "venue": "ICLR2021",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Codebert: A pre-trained model for programming and natural languages": {
        "type": "article",
        "key": "feng2020codebert",
        "title": "Codebert: A pre-trained model for programming and natural languages",
        "author": "Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others",
        "journal": "arXiv preprint arXiv:2002.08155",
        "year": "2020",
        "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.",
        "venue": "EMNLP2020",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Neural code comprehension: A learnable representation of code semantics": {
        "type": "article",
        "key": "ben2018neural",
        "title": "Neural code comprehension: A learnable representation of code semantics",
        "author": "Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten",
        "journal": "Advances in neural information processing systems",
        "volume": "31",
        "year": "2018",
        "abstract": "With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human-and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data-and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",
        "venue": "NeurIPS2018",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Meta large language model compiler: Foundation models of compiler optimization": {
        "type": "article",
        "key": "cummins2024meta",
        "title": "Meta large language model compiler: Foundation models of compiler optimization",
        "author": "Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh",
        "journal": "arXiv preprint arXiv:2407.02524",
        "year": "2024",
        "venue": "Meta2024",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
        "keywords": [
            "code model",
            "IR code model",
            "code generation",
            "compiler optimization"
        ]
    },
    "Symmetry-Preserving Program Representations for Learning Code Semantics": {
        "type": "article",
        "key": "pei2023symmetry",
        "title": "Symmetry-Preserving Program Representations for Learning Code Semantics",
        "author": "Pei, Kexin and Li, Weichen and Jin, Qirui and Liu, Shuyang and Geng, Scott and Cavallaro, Lorenzo and Yang, Junfeng and Jana, Suman",
        "journal": "arXiv preprint arXiv:2308.03312",
        "year": "2023",
        "venue": "ICML2024",
        "abstract": "Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures. Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.",
        "keywords": [
            "code model",
            "source code model"
        ]
    },
    "Fair: Flow type-aware pre-training of compiler intermediate representations": {
        "type": "inproceedings",
        "key": "niu2024fair",
        "title": "Fair: Flow type-aware pre-training of compiler intermediate representations",
        "author": "Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin",
        "booktitle": "Proceedings of the 46th IEEE/ACM International Conference on Software Engineering",
        "pages": "1--12",
        "year": "2024",
        "abstract": "While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.",
        "venue": "ICSE2024",
        "keywords": [
            "code model",
            "IR code model"
        ]
    },
    "How could neural networks understand programs?": {
        "type": "inproceedings",
        "key": "peng2021could",
        "title": "How could neural networks understand programs?",
        "author": "Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan",
        "booktitle": "International Conference on Machine Learning",
        "pages": "8476--8486",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (ie, the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at:\\url {https://github. com/pdlan/OSCAR}.",
        "keywords": [
            "code model",
            "IR code model"
        ]
    },
    "Programl: A graph-based program representation for data flow analysis and compiler optimizations": {
        "type": "inproceedings",
        "key": "cummins2021programl",
        "title": "Programl: A graph-based program representation for data flow analysis and compiler optimizations",
        "author": "Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O\u2019Boyle, Michael FP and Leather, Hugh",
        "booktitle": "International Conference on Machine Learning",
        "pages": "2244--2253",
        "year": "2021",
        "organization": "PMLR",
        "venue": "ICML2021",
        "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML-Program Graphs for Machine Learning-a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
        "keywords": [
            "static analysis",
            "fundamental analysis",
            "compiler optimization",
            "code model",
            "IR code model"
        ]
    },
    "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries": {
        "type": "inproceedings",
        "key": "xie2024resym",
        "title": "ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries",
        "author": "Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu",
        "booktitle": "Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)",
        "year": "2024",
        "venue": "CCS2024",
        "abstract": "Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, eg, recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the inherent token limitations in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.",
        "keywords": [
            "code model",
            "binary code model",
            "code generation",
            "program decompilation"
        ]
    },
    "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases": {
        "type": "article",
        "key": "su2024source",
        "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Kaiyuan and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2405.19581",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Codeart: Better code models by attention regularization when symbols are lacking": {
        "type": "article",
        "key": "su2024codeart",
        "title": "Codeart: Better code models by attention regularization when symbols are lacking",
        "author": "Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Zhuo and Ye, Yapeng and Huang, Jianjun and Zhang, Xiangyu",
        "journal": "Proceedings of the ACM on Software Engineering",
        "volume": "1",
        "number": "FSE",
        "pages": "562--585",
        "year": "2024",
        "publisher": "ACM New York, NY, USA",
        "venue": "FSE2024",
        "abstract": "Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Lmpa: Improving decompilation by synergy of large language model and program analysis": {
        "type": "article",
        "key": "xu2023lmpa",
        "title": "Lmpa: Improving decompilation by synergy of large language model and program analysis",
        "author": "Xu, Xiangzhe and Zhang, Zhuo and Feng, Shiwei and Ye, Yapeng and Su, Zian and Jiang, Nan and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu",
        "journal": "arXiv preprint arXiv:2306.02546",
        "year": "2023",
        "venue": "arXiv2023",
        "abstract": "Decompilation aims to recover the source code form of a binary executable. It has many applications in security and software engineering such as malware analysis, vulnerability detection and code reuse. A prominent challenge in decompilation is to recover variable names. We propose a novel method that leverages the synergy of large language model (LLM) and program analysis. Language models encode rich multi-modal knowledge, but its limited input size prevents providing sufficient global context for name recovery. We propose to divide the task to many LLM queries and use program analysis to correlate and propagate the query results, which in turn improves the performance of LLM by providing additional contextual information. Our results show that 75% of the recovered names are considered good by users and our technique outperforms the state-of-the-art technique by 16.5% and 20.23% in precision and recall, respectively.",
        "keywords": [
            "code model",
            "binary code model"
        ]
    },
    "Jtrans: Jump-aware transformer for binary code similarity detection": {
        "type": "inproceedings",
        "key": "wang2022jtrans",
        "title": "Jtrans: Jump-aware transformer for binary code similarity detection",
        "author": "Wang, Hao and Qu, Wenjie and Katz, Gilad and Zhu, Wenyu and Gao, Zeyu and Qiu, Han and Zhuge, Jianwei and Zhang, Chao",
        "booktitle": "Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "pages": "1--13",
        "year": "2022",
        "venue": "ISSTA2022",
        "abstract": "Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines.",
        "keywords": [
            "static analysis",
            "binary code model",
            "code model",
            "binary code model"
        ]
    },
    "Swe-bench: Can language models resolve real-world github issues?": {
        "type": "article",
        "key": "jimenez2023swe",
        "title": "Swe-bench: Can language models resolve real-world github issues?",
        "author": "Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik",
        "journal": "arXiv preprint arXiv:2310.06770",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "keywords": [
            "benchmark",
            "code generation",
            "program repair"
        ]
    },
    "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories": {
        "type": "article",
        "key": "li2024evocodebench",
        "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
        "author": "Li, Jia and Li, Ge and Zhang, Xuanming and Dong, Yihong and Jin, Zhi",
        "journal": "arXiv preprint arXiv:2404.00599",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",
        "keywords": [
            "benchmark",
            "code generation"
        ]
    },
    "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks": {
        "type": "article",
        "key": "xie2024codebenchgen",
        "title": "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks",
        "author": "Xie, Yiqing and Xie, Alex and Sheth, Divyanshu and Liu, Pengfei and Fried, Daniel and Rose, Carolyn",
        "journal": "arXiv preprint arXiv:2404.00566",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will release the code of both the framework and the dataset upon acceptance.",
        "keywords": [
            "code generation",
            "benchmark"
        ]
    },
    "A Survey on Large Language Models for Code Generation": {
        "type": "article",
        "key": "jiang2024survey",
        "title": "A Survey on Large Language Models for Code Generation",
        "author": "Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun",
        "journal": "arXiv preprint arXiv:2406.00515",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (this https URL) to continuously document and disseminate the most recent advances in the field.",
        "keywords": [
            "survey",
            "code generation"
        ]
    },
    "Is Self-Repair a Silver Bullet for Code Generation?": {
        "type": "inproceedings",
        "key": "olausson2023self",
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "author": "Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "year": "2023",
        "venue": "ICLR2023",
        "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
        "keywords": [
            "code generation",
            "program repair"
        ]
    },
    "Repairagent: An autonomous, llm-based agent for program repair": {
        "type": "article",
        "key": "bouzenia2024repairagent",
        "title": "Repairagent: An autonomous, llm-based agent for program repair",
        "author": "Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael",
        "journal": "arXiv preprint arXiv:2403.17134",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.",
        "keywords": [
            "code generation",
            "program repair",
            "agent design",
            "planning"
        ]
    },
    "Natural Language Commanding via Program Synthesis": {
        "type": "article",
        "key": "gandhi2023natural",
        "title": "Natural Language Commanding via Program Synthesis",
        "author": "Gandhi, Apurva and Nguyen, Thong Q and Jiao, Huitian and Steen, Robert and Bhatawdekar, Ameya",
        "journal": "arXiv preprint arXiv:2306.03460",
        "year": "2023",
        "venue": "Microsoft2023",
        "abstract": "We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Effective Large Language Model Debugging with Best-first Tree Search": {
        "type": "article",
        "key": "song2024effective",
        "title": "Effective Large Language Model Debugging with Best-first Tree Search",
        "author": "Song, Jialin and Raiman, Jonathan and Catanzaro, Bryan",
        "journal": "arXiv preprint arXiv:2407.19055",
        "year": "2024",
        "venue": "NVDIA2024",
        "abstract": "Large Language Models (LLMs) show promise in code generation tasks. However, their code-writing abilities are often limited in scope: while they can successfully implement simple functions, they struggle with more complex tasks. A fundamental difference with how an LLM writes code, compared to a human programmer, is that it cannot consistently spot and fix bugs. Debugging is a crucial skill for programmers and it enables iterative code refinement towards a correct implementation. In this work, we propose a novel algorithm to enable LLMs to debug their code via self-reflection and search where a model attempts to identify its previous mistakes. Our key contributions are 1) a best-first tree search algorithm with self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains its superiority when we measure pass rates taking into account additional inference costs incurred by tree search. 2) A novel interpretability study on what self-reflections attend to in buggy programs and how they impact bug fixes, which provides a deeper understanding of the debugging process. 3) An extensive study on when self-reflections are effective in finding bugs.",
        "keywords": [
            "code generation",
            "debugging"
        ]
    },
    "Automatic Programming: Large Language Models and Beyond": {
        "type": "article",
        "key": "lyu2024automatic",
        "title": "Automatic Programming: Large Language Models and Beyond",
        "author": "Lyu, Michael R and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon",
        "journal": "arXiv preprint arXiv:2405.02213",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance.",
        "keywords": [
            "general coding task",
            "empirical study"
        ]
    },
    "Verified multi-step synthesis using large language models and monte carlo tree search": {
        "type": "article",
        "key": "brandfonbrener2024verified",
        "title": "Verified multi-step synthesis using large language models and monte carlo tree search",
        "author": "Brandfonbrener, David and Raja, Sibi and Prasad, Tarun and Loughridge, Chloe and Yang, Jianang and Henniger, Simon and Byrd, William E and Zinkov, Robert and Amin, Nada",
        "journal": "arXiv preprint arXiv:2402.08147",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "Hypothesis search: Inductive reasoning with language models": {
        "type": "article",
        "key": "wang2023hypothesis",
        "title": "Hypothesis search: Inductive reasoning with language models",
        "author": "Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D",
        "journal": "arXiv preprint arXiv:2309.05660",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding \"in context learning.\" This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ]
    },
    "Guess \\& Sketch: Language Model Guided Transpilation": {
        "type": "article",
        "key": "lee2023guess",
        "title": "Guess \\& Sketch: Language Model Guided Transpilation",
        "author": "Lee, Celine and Mahmoud, Abdulrahman and Kurek, Michal and Campanoni, Simone and Brooks, David and Chong, Stephen and Wei, Gu-Yeon and Rush, Alexander M",
        "journal": "arXiv preprint arXiv:2309.14396",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Verified Code Transpilation with LLMs": {
        "type": "article",
        "key": "bhatia2024verified",
        "title": "Verified Code Transpilation with LLMs",
        "author": "Bhatia, Sahil and Qiu, Jie and Hasabnis, Niranjan and Seshia, Sanjit A and Cheung, Alvin",
        "journal": "arXiv preprint arXiv:2406.03003",
        "year": "2024",
        "venue": "NeurIPS2024",
        "abstract": "Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for {\\em four different} DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.",
        "keywords": [
            "code generation",
            "program synthesis",
            "static analysis",
            "program verification"
        ]
    },
    "Rectifier: Code translation with corrector via llms": {
        "type": "article",
        "key": "yin2024rectifier",
        "title": "Rectifier: Code translation with corrector via llms",
        "author": "Yin, Xin and Ni, Chao and Nguyen, Tien N and Wang, Shaohua and Yang, Xiaohu",
        "journal": "arXiv preprint arXiv:2407.07472",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Software migration is garnering increasing attention with the evolution of software and society. Early studies mainly relied on handcrafted translation rules to translate between two languages, the translation process is error-prone and time-consuming. In recent years, researchers have begun to explore the use of pre-trained large language models (LLMs) in code translation. However, code translation is a complex task that LLMs would generate mistakes during code translation, they all produce certain types of errors when performing code translation tasks, which include (1) compilation error, (2) runtime error, (3) functional error, and (4) non-terminating execution. We found that the root causes of these errors are very similar (e.g. failure to import packages, errors in loop boundaries, operator errors, and more). In this paper, we propose a general corrector, namely Rectifier, which is a micro and universal model for repairing translation errors. It learns from errors generated by existing LLMs and can be widely applied to correct errors generated by any LLM. The experimental results on translation tasks between C++, Java, and Python show that our model has effective repair ability, and cross experiments also demonstrate the robustness of our method.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Learning performance-improving code edits": {
        "type": "article",
        "key": "shypula2023learning",
        "title": "Learning performance-improving code edits",
        "author": "Shypula, Alexander and Madaan, Aman and Zeng, Yimeng and Alon, Uri and Gardner, Jacob and Hashemi, Milad and Neubig, Graham and Ranganathan, Parthasarathy and Bastani, Osbert and Yazdanbakhsh, Amir",
        "journal": "arXiv preprint arXiv:2302.07867",
        "year": "2023",
        "venue": "ICLR2024",
        "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77K competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\". To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves an average speedup of 5.65X on CodeLlama-13B and 6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Enabling Memory Safety of C Programs using LLMs": {
        "type": "article",
        "key": "mohammed2024enabling",
        "title": "Enabling Memory Safety of C Programs using LLMs",
        "author": "Mohammed, Nausheen and Lal, Akash and Rastogi, Aseem and Roy, Subhajit and Sharma, Rahul",
        "journal": "arXiv preprint arXiv:2404.01096",
        "year": "2024",
        "venue": "arXiv2024",
        "abstract": "Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Refactoring programs using large language models with few-shot examples": {
        "type": "inproceedings",
        "key": "shirafuji2023refactoring",
        "title": "Refactoring programs using large language models with few-shot examples",
        "author": "Shirafuji, Atsushi and Oda, Yusuke and Suzuki, Jun and Morishita, Makoto and Watanobe, Yutaka",
        "booktitle": "2023 30th Asia-Pacific Software Engineering Conference (APSEC)",
        "pages": "151--160",
        "year": "2023",
        "organization": "IEEE",
        "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Further-more, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.",
        "venue": "APSEC2023",
        "keywords": [
            "code generation",
            "program transformation"
        ]
    },
    "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search": {
        "type": "article",
        "key": "DBLP:journals/corr/abs-2405-15383",
        "author": "Nicola Dainese and Matteo Merler and Minttu Alakuijala and Pekka Marttinen",
        "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
        "journal": "CoRR",
        "volume": "abs/2405.15383",
        "year": "2024",
        "url": "https://doi.org/10.48550/arXiv.2405.15383",
        "doi": "10.48550/ARXIV.2405.15383",
        "eprinttype": "arXiv",
        "eprint": "2405.15383",
        "timestamp": "Wed, 19 Jun 2024 08:52:55 +0200",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2405-15383.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "NeurIPS2024",
        "abstract": "In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules": {
        "type": "inproceedings",
        "key": "DBLP:conf/iclr/LeCSGSJ24",
        "author": "Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty",
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "booktitle": "The Twelfth International Conference on Learning Representations",
        "publisher": "OpenReview.net",
        "year": "2024",
        "url": "https://openreview.net/forum?id=vYhglxSj8j",
        "timestamp": "Wed, 07 Aug 2024 17:11:53 +0200",
        "biburl": "https://dblp.org/rec/conf/iclr/LeCSGSJ24.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICLR2024",
        "abstract": "Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContests. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain's success.",
        "keywords": [
            "code generation",
            "program synthesis"
        ]
    },
    "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/GuoXD0M23",
        "author": "Daya Guo and Canwen Xu and Nan Duan and Jian Yin and Julian J. McAuley",
        "title": "LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "12098--12107",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/guo23j.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:08 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/GuoXD0M23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens-bridge tokens and memory tokens-to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.",
        "keywords": [
            "code generation",
            "code completion",
            "code model",
            "source code model"
        ]
    },
    "Repository-Level Prompt Generation for Large Language Models of Code": {
        "type": "inproceedings",
        "key": "DBLP:conf/icml/ShrivastavaLT23",
        "author": "Disha Shrivastava and Hugo Larochelle and Daniel Tarlow",
        "title": "Repository-Level Prompt Generation for Large Language Models of Code",
        "booktitle": "International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA",
        "series": "Proceedings of Machine Learning Research",
        "volume": "202",
        "pages": "31693--31715",
        "publisher": "PMLR",
        "year": "2023",
        "url": "https://proceedings.mlr.press/v202/shrivastava23a.html",
        "timestamp": "Mon, 28 Aug 2023 17:23:09 +0200",
        "biburl": "https://dblp.org/rec/conf/icml/ShrivastavaLT23.bib",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "venue": "ICML2023",
        "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines.",
        "keywords": [
            "code generation",
            "code completion",
            "prompting strategy",
            "RAG"
        ]
    }
}